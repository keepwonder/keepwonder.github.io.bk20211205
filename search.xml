<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[sklearn in action]]></title>
    <url>%2Funcategorized%2Fsklearn-in-action-with-Decistion-Tree%2F</url>
    <content type="text"><![CDATA[决策树]]></content>
      <tags>
        <tag>MLDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Getting Stared]]></title>
    <url>%2Funcategorized%2Felastic-get-started%2F</url>
    <content type="text"><![CDATA[简介安装Elastic 安装elastic之前需要检查是否安装Java环境，Java版本至少是Java8 1234$ java -versionjava version &quot;1.8.0_144&quot;Java(TM) SE Runtime Environment (build 1.8.0_144-b01)Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) 安装elastic，最简单的方法是下载压缩包，并解压 123$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.2.tar.gz$ tar -xvf elasticsearch-6.2.2.tar.gz$ cd elasticsearch-6.2.2/ 进入解压后的目录，运行如下命令，启动elastic 123$ ./bin/elasticsearch或者$ ./bin/elasticsearch -d -p pid 运行正常会出现如下界面，默认运行在9200端口 12345678910111213141516171819202122232425262728293031$ ./bin/elasticsearch[2018-02-26T13:23:32,399][INFO ][o.e.n.Node ] [] initializing ...[2018-02-26T13:23:32,455][INFO ][o.e.e.NodeEnvironment ] [Ftxy5_l] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [133.8gb], net total_space [232.6gb], types [hfs][2018-02-26T13:23:32,456][INFO ][o.e.e.NodeEnvironment ] [Ftxy5_l] heap size [989.8mb], compressed ordinary object pointers [true][2018-02-26T13:23:32,457][INFO ][o.e.n.Node ] node name [Ftxy5_l] derived from node ID [Ftxy5_lYRBa67IuaMyOOBw]; set [node.name] to override[2018-02-26T13:23:32,457][INFO ][o.e.n.Node ] version[6.2.2], pid[2242], build[10b1edd/2018-02-16T19:01:30.685723Z], OS[Mac OS X/10.12.6/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_144/25.144-b01][2018-02-26T13:23:32,458][INFO ][o.e.n.Node ] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/var/folders/yf/m8nhcn4x1vd1snmnykt7n89h0000gn/T/elasticsearch.c7Lp6Ain, -XX:+HeapDumpOnOutOfMemoryError, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Des.path.home=/Users/jockie/install_programs/elasticsearch-6.2.2, -Des.path.conf=/Users/jockie/install_programs/elasticsearch-6.2.2/config][2018-02-26T13:23:32,950][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [aggs-matrix-stats][2018-02-26T13:23:32,950][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [analysis-common][2018-02-26T13:23:32,950][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [ingest-common][2018-02-26T13:23:32,950][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [lang-expression][2018-02-26T13:23:32,950][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [lang-mustache][2018-02-26T13:23:32,950][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [lang-painless][2018-02-26T13:23:32,950][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [mapper-extras][2018-02-26T13:23:32,951][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [parent-join][2018-02-26T13:23:32,951][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [percolator][2018-02-26T13:23:32,951][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [rank-eval][2018-02-26T13:23:32,951][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [reindex][2018-02-26T13:23:32,951][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [repository-url][2018-02-26T13:23:32,951][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [transport-netty4][2018-02-26T13:23:32,951][INFO ][o.e.p.PluginsService ] [Ftxy5_l] loaded module [tribe][2018-02-26T13:23:32,952][INFO ][o.e.p.PluginsService ] [Ftxy5_l] no plugins loaded[2018-02-26T13:23:34,693][INFO ][o.e.d.DiscoveryModule ] [Ftxy5_l] using discovery type [zen][2018-02-26T13:23:35,115][INFO ][o.e.n.Node ] initialized[2018-02-26T13:23:35,115][INFO ][o.e.n.Node ] [Ftxy5_l] starting ...[2018-02-26T13:23:35,226][INFO ][o.e.t.TransportService ] [Ftxy5_l] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[2018-02-26T13:23:38,273][INFO ][o.e.c.s.MasterService ] [Ftxy5_l] zen-disco-elected-as-master ([0] nodes joined), reason: new_master &#123;Ftxy5_l&#125;&#123;Ftxy5_lYRBa67IuaMyOOBw&#125;&#123;wcDZCO9STL-OfnTbGDNI_g&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;[2018-02-26T13:23:38,277][INFO ][o.e.c.s.ClusterApplierService] [Ftxy5_l] new_master &#123;Ftxy5_l&#125;&#123;Ftxy5_lYRBa67IuaMyOOBw&#125;&#123;wcDZCO9STL-OfnTbGDNI_g&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, reason: apply cluster state (from master [master &#123;Ftxy5_l&#125;&#123;Ftxy5_lYRBa67IuaMyOOBw&#125;&#123;wcDZCO9STL-OfnTbGDNI_g&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125; committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)]])[2018-02-26T13:23:38,290][INFO ][o.e.h.n.Netty4HttpServerTransport] [Ftxy5_l] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[2018-02-26T13:23:38,290][INFO ][o.e.n.Node ] [Ftxy5_l] started[2018-02-26T13:23:38,295][INFO ][o.e.g.GatewayService ] [Ftxy5_l] recovered [0] indices into cluster_state 打开另一个terminal，运行以下命令，查看说明信息 123456789101112131415161718192021$ http http://127.0.0.1:9200HTTP/1.1 200 OKcontent-encoding: gzipcontent-length: 280content-type: application/json; charset=UTF-8&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;cluster_uuid&quot;: &quot;IkkdQBGTSIGucxbOrB2OiA&quot;, &quot;name&quot;: &quot;Ftxy5_l&quot;, &quot;tagline&quot;: &quot;You Know, for Search&quot;, &quot;version&quot;: &#123; &quot;build_date&quot;: &quot;2018-02-16T19:01:30.685723Z&quot;, &quot;build_hash&quot;: &quot;10b1edd&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;7.2.1&quot;, &quot;minimum_index_compatibility_version&quot;: &quot;5.0.0&quot;, &quot;minimum_wire_compatibility_version&quot;: &quot;5.6.0&quot;, &quot;number&quot;: &quot;6.2.2&quot; &#125;&#125; http http://127.0.0.1:9200 命令请求9200端口，elastic返回一个JSON对象，包含当前集群、节点、版本等信息。 停止elastic服务终端按下Ctrl+C，terminal打印信息如下 或者使用命令`kill pid’ 1234^C[2018-02-26T13:31:31,983][INFO ][o.e.n.Node ] [Ftxy5_l] stopping ...[2018-02-26T13:31:32,001][INFO ][o.e.n.Node ] [Ftxy5_l] stopped[2018-02-26T13:31:32,001][INFO ][o.e.n.Node ] [Ftxy5_l] closing ...[2018-02-26T13:31:32,008][INFO ][o.e.n.Node ] [Ftxy5_l] closed 允许其他机器访问默认情况下，Elastic 只允许本机访问，如果需要远程访问，可以修改 Elastic 安装目录的config/elasticsearch.yml文件，去掉network.host的注释，将它的值改成0.0.0.0，然后重新启动 Elastic。 123$ vi config/elasticsearch.yml55 #network.host: 192.168.0.156 network.host: 0.0.0.0 安装kibanakibana安装方法和elastic类似 下载解压包并解压 1234$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.2.2-darwin-x86_64.tar.gz$ tar -xvf kibana-6.2.2-darwin-x86_64.tar.gz$ mv kibana-6.2.2-darwin-x86_64.tar.gz kibana-6.2.2$ cd kibana-6.2.2 修改配置文件 1$ vi config/kibana.yml 运行服务 1$ ./bin/kibana 使用kibana打开浏览器，输入http://localhost:5601 基本概念 Near Realtime (NRT) Elasticsearch是一个几乎实时的搜索平台。 Cluster &amp; Node Elastic本质上是一个分布式数据库，可以运行在多台服务器上，每台服务器可以运行多个Elastic实例。一个Elastic实例称为一个节点（node），多个节点则组成一个集群（cluster） Index 和关系型数据库的数据库的概念一样，多条Document构成一个Index Type 按照规划 Elastic6.x版本已经不推荐使用，后面版本将会彻底移除 Document Index里面的单条记录 Shards &amp; Replicas 分片和副本 Exploring Your ClusterElasticsearch通过REST API接口来操作 Check your cluster, node, and index health, status, and statistics Administer your cluster, node, and index data and metadata Perform CRUD (Create, Read, Update, and Delete) and search operations against your indexes Execute advanced search operations such as paging, sorting, filtering, scripting, aggregations, and many others 查看集群的健康状况123$ curl -XGET &apos;localhost:9200/_cat/health?v&amp;pretty&apos;epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1519625839 14:17:19 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0% 通过以上可以看出，名为’elasticsearch’的集群正常运行，且状态为greenelasticsearch的健康状态分为：green，yellow，red Green - everything is good (cluster is fully functional) Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional) Red - some data is not available for whatever reason (cluster is partially functional) 获取所有的节点信息123$ curl -XGET &apos;localhost:9200/_cat/nodes?v&amp;pretty&apos;ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name192.168.0.101 28 74 5 1.77 mdi * Ftxy5_l 列出所有的index12$ curl -XGET &apos;localhost:9200/_cat/indices?v&amp;pretty&apos;health status index uuid pri rep docs.count docs.deleted store.size pri.store.size 以上说明，集群中目前还没有index 新建Index以上操作，是在terminal下操作，接下来使用kibana console操作PUT /customer?prettyconsole展示如下： Index and Query a DocumentIndex1234567891011121314151617181920请求：PUT /customer/_doc/1?pretty&#123; &quot;name&quot;:&quot;John Doe&quot;&#125;响应：&#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1&#125; Query1234567891011121314请求：GET /customer/_doc/1?pretty响应：&#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;John Doe&quot; &#125;&#125; 删除一个Index123456请求：DELETE /customer?pretty响应：&#123; &quot;acknowledged&quot;: true&#125; RESTAPI格式: &lt;REST Verb&gt; /&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt; PUT /customer PUT /customer/_doc/1{ “name”: “Jone Doe”} GET /customer/_doc/1 DELETE /customer 修改数据Indexing/Replacing Documents12345678910111213141516171819PUT /customer/_doc/1?pretty&#123; &quot;name&quot;: &quot;John Doe&quot;&#125;PUT /customer/_doc/1?pretty&#123; &quot;name&quot;: &quot;Jane Doe&quot;&#125;PUT /customer/_doc/2?pretty&#123; &quot;name&quot;: &quot;Jane Doe&quot;&#125;POST /customer/_doc?pretty&#123; &quot;name&quot;: &quot;Jane Doe&quot;&#125; Updating Documents1234567891011121314POST /customer/_doc/1/_update?pretty&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot; &#125;&#125;POST /customer/_doc/1/_update?pretty&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot;, &quot;age&quot;: 20 &#125;&#125;POST /customer/_doc/1/_update?pretty&#123; &quot;script&quot; : &quot;ctx._source.age += 5&quot;&#125; Deleting Documents1DELETE /customer/_doc/2?pretty Batch Processing12345POST /customer/_doc/_bulk?pretty&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;John Doe&quot; &#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;Jane Doe&quot; &#125; 1234POST /customer/_doc/_bulk?pretty&#123;&quot;update&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;doc&quot;: &#123; &quot;name&quot;: &quot;John Doe becomes Jane Doe&quot; &#125; &#125;&#123;&quot;delete&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125; Exploring Your DataSample DatasetLoading the Sample Dataset12345$ curl -H &quot;Content-Type: application/json&quot; -XPOST &quot;localhost:9200/customer/_doc/_bulk?pretty&amp;refresh&quot; --data-binary &quot;@accounts.json&quot;$ curl &quot;localhost:9200/_cat/indices?v&quot;health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open customer ReKMezdYSWGfIvOUdMrBuQ 5 1 1000 0 478.2kb 478.2kb The Search APIIntroducing the Query LanguageExecuting SearchesExecuting FiltersExecuting Aggregations总结Elasticsearch是一个既简单有复杂的产品，通过REST APIs操作数据，返回json格式的数据。]]></content>
  </entry>
  <entry>
    <title><![CDATA[redis quickstart]]></title>
    <url>%2Fpython%2Fredis-quickstart%2F</url>
    <content type="text"><![CDATA[Redis简介Redis键 KEYS pattern 查找所有符合给定模式 pattern 的 key 。 EXISTS key 检查给定 key 是否存在。 EXPIRE key seconds 为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 PEXPIRE key milliseconds 这个命令和 EXPIRE 命令的作用类似，但是它以毫秒为单位设置 key 的生存时间，而不像 EXPIRE 命令那样，以秒为单位。 PERSIST key 移除给定 key 的生存时间，将这个 key 从『易失的』(带生存时间 key )转换成『持久的』(一个不带生存时间、永不过期的 key )。 RANDOMKEY 从当前数据库中随机返回(不删除)一个 key 。 TTL key 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 PTTL key 这个命令类似于 TTL 命令，但它以毫秒为单位返回 key 的剩余生存时间，而不是像 TTL 命令那样，以秒为单位。 RENAME key newkey 将 key 改名为 newkey 。当 key 和 newkey 相同，或者 key 不存在时，返回一个错误。当 newkey 已经存在时， RENAME 命令将覆盖旧值。 RENAMENX key newkey 当且仅当 newkey 不存在时，将 key 改名为 newkey 。当 key 不存在时，返回一个错误。 TYPE key 返回 key 所储存的值的类型。 DEL key [key …] 删除给定的一个或多个 key 。不存在的 key 会被忽略。 FLUSHALL 删除所有的key Redis数据类型 String(字符串) Hash(哈希表) List(列表) Set(集合) SortedSet(有序集合) 字符串操作 SET key value [EX seconds] [PX milliseconds] [NX|XX] 将字符串值 value 关联到 key 。如果 key 已经持有其他值， SET 就覆写旧值，无视类型。对于某个原本带有生存时间（TTL）的键来说， 当 SET 命令成功在这个键上执行时， 这个键原有的 TTL 将被清除。 MSET key value [key value …] 同时设置一个或多个 key-value 对。 GET key 返回 key 所关联的字符串值。如果 key 不存在那么返回特殊值 nil 。假如 key 储存的值不是字符串类型，返回一个错误，因为 GET 只能用于处理字符串值。 MGET key [key …] 返回所有(一个或多个)给定 key 的值。 GETRANGE key start end 返回 key 中字符串值的子字符串，字符串的截取范围由 start 和 end 两个偏移量决定(包括 start 和 end 在内)。 负数偏移量表示从字符串最后开始计数， -1 表示最后一个字符， -2 表示倒数第二个，以此类推。 GETRANGE 通过保证子字符串的值域(range)不超过实际字符串的值域来处理超出范围的值域请求。 GETSET key value 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 当 key 存在但不是字符串类型时，返回一个错误。 APPEND key value 如果 key 已经存在并且是一个字符串， APPEND 命令将 value 追加到 key 原来的值的末尾。如果 key 不存在， APPEND 就简单地将给定 key 设为 value ，就像执行 SET key value 一样。 STRLEN key 返回 key 所储存的字符串值的长度。 DECR key 将 key 中储存的数字值减一。 DECRBY key decrement 将 key 所储存的值减去减量 decrement 。 INCR key 将 key 中储存的数字值增一。 INCRBY key increment 将 key 所储存的值加上增量 increment 。 INCRBYFLOAT key increment 为 key 中所储存的值加上浮点数增量 increment 。 哈希表操作 HSET key field value 将哈希表 key 中的域 field 的值设为 value 。 如果 key 不存在，一个新的哈希表被创建并进行 HSET 操作。 如果域 field 已经存在于哈希表中，旧值将被覆盖。 HGET key field 返回哈希表 key 中给定域 field 的值。 HDEL key field [field …] 删除哈希表 key 中的一个或多个指定域，不存在的域将被忽略。 HKEYS key 返回哈希表 key 中的所有域。 HVALS key 返回哈希表 key 中所有域的值。 HLEN key 返回哈希表 key 中域的数量。 HEXISTS key field 查看哈希表 key 中，给定域 field 是否存在。 HMSET key field value [field value …] 同时将多个 field-value (域-值)对设置到哈希表 key 中。 此命令会覆盖哈希表中已存在的域。 如果 key 不存在，一个空哈希表被创建并执行 HMSET 操作。 列表操作 LPUSH key value [value …] 将一个或多个值 value 插入到列表 key 的表头 LRANGE key start stop 返回列表 key 中指定区间内的元素，区间以偏移量 start 和 stop 指定。 LLEN key 返回列表 key 的长度。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Road to Python Full Stack]]></title>
    <url>%2Fpython%2FRoad-to-Python-Full-Stack%2F</url>
    <content type="text"><![CDATA[python全栈之路简单介绍这篇文章记录一下，用Python做全栈开发的一些个人浅显感想。首先python全栈有两个关键词，一个是python,一个是全栈。 首先说python: 什么是python？python是一种脚本语言，一种解释型语言，一种胶水语言。作为脚本语言，python可以写日常用的一些脚本任务；作为解释型语言，python可以直接通过命令行，验证自己的想法；作为胶水语言，python可以在程序中整合其他语言的脚本，比如C、Java等。 python有什么优势？python语言简单优雅，容易入门，可以通过很短的代码实现复杂的功能。python内置许多标准函数和标准库，同时python拥有丰富的第三方库，可以应用在众多领域。 python的主要应用领域？python语言简单，但不代表只能做简单的事情。从日常的脚本程序，web开发，爬虫开发到数据分析，机器学习，人工智能等领域，python都有丰富的库支持。 再说全栈：英文：Full Stack，传统的开发分为前端和后端，而全栈就是结合前后端外加上各种相关知识，所以python全栈，不仅要会python相关领域的知识，同时也要会前端知识，比如html,css,javascript等前端必备知识。 必备知识 python 基础 python 高级 python Web开发 python 爬虫 python 数据分析 说明：以上每个主题都涉及到很多内容，后期会按主题来更新相关内容，既是整理所学知识，同时也是巩固所学知识，以期以后能够熟练的运用所学知识，达到学有所用。感想：深处当今这个互联网时代，知识爆发增长，学习永无止尽，我们不能为了学习而学习，摆脱知识焦虑，我们需要带着目标去学习，这样我们才会有一个明确的方向，不然就会淹没在这知识的海洋中。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HomeBrew简单使用]]></title>
    <url>%2Fmac%2FHomeBrew%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[HomeBrew是什么?详情见官网HomeBrew是一款Mac OS平台下的软件管理工具，可以通过简单的命令安装、卸载、升级、管理软件。 安装HomeBrew1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 在终端输入以上命令，根据提示进行操作。 HomeBrew安装完成后，会在/usr/local下生成一个HomeBrew目录，1/usr/local/Homebrew 提供一个brew命令在/HomeBrew/bin目录下，12jockie:/usr/local/Homebrew/bin (stable)$ lsbrew 并且在/usr/local/bin下生成一个链接12jockie:/usr/local/bin$ ll brewlrwxr-xr-x 1 jockie admin 28B Jun 3 2017 brew -&gt; /usr/local/Homebrew/bin/brew brew管理软件 查看brew命令123456789101112131415161718192021222324jockie:~$ brewExample usage: brew search [TEXT|/REGEX/] brew (info|home|options) [FORMULA...] brew install FORMULA... brew update brew upgrade [FORMULA...] brew uninstall FORMULA... brew list [FORMULA...]Troubleshooting: brew config brew doctor brew install -vd FORMULADevelopers: brew create [URL [--no-fetch]] brew edit [FORMULA...] https://docs.brew.sh/Formula-Cookbook.htmlFurther help: man brew brew help [COMMAND] brew home 常用命令安装软件: brew install PKG_NAME卸载软件: brew uninstall PKG_NAME升级软件: brew upgrade PKG_NAME更新brew: brew update查看帮助: brew help 说明：具体哪条命令不清楚，可以用帮助命令，比如查看update怎么使用，有没有什么参数：12345678910jockie:~$ brew update --helpbrew update [--merge] [--force]: Fetch the newest version of Homebrew and all formulae from GitHub using git(1) and perform any necessary migrations. If --merge is specified then git merge is used to include updates (rather than git rebase). If --force (or -f) is specified then always do a slower, full update check even if unnecessary. 或者12345678910jockie:~$ brew help updatebrew update [--merge] [--force]: Fetch the newest version of Homebrew and all formulae from GitHub using git(1) and perform any necessary migrations. If --merge is specified then git merge is used to include updates (rather than git rebase). If --force (or -f) is specified then always do a slower, full update check even if unnecessary. 总结通过brew方式，使我们在macOS系统下能够方便的管理软件。在使用brew的过程中，遇到不熟悉的命令，最简单的方式就是通过help方式查看具体操作信息。]]></content>
      <categories>
        <category>mac</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云python3+flask部署]]></title>
    <url>%2Fweb%2F%E9%98%BF%E9%87%8C%E4%BA%91python3-flask%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[网站根目录普通www用户pip install gunicornvim gunicornmkdir logs root用户apt-get install supervisor]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memcached简单使用]]></title>
    <url>%2Fdatabase%2Fmemcached%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是memcachedMemcached 是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提高动态、数据库驱动网站的速度。Memcached基于一个存储键/值对的hashmap。其守护进程（daemon ）是用C写的，但是客户端可以用任何语言来编写，并通过memcached协议与守护进程通信。 安装memcachedmac下安装1brew install memcached 查看是否安装成功1234jockie:~$ which memcached/usr/local/bin/memcachedjockie:~$ brew list | grep memcachedmemcached ubuntu下安装1sudo apt-get install memcached 查看是否安装成功12:~$ which memcached /usr/bin/memcached 启动memcached1jockie:~$ memcached -d -p 指定占用端口，默认11211 -d 让memcached在后台运行 -m 指定占用内存，单位为MB,默认64MB -l 如果想要让别的机器连接，就必须设置-l 0.0.0.0 查看服务是否启动123jockie:~$ ps aux | grep memcachedjockie 4980 0.0 0.0 2442020 1992 s001 S+ 7:12PM 0:00.00 grep memcachedjockie 4974 0.0 0.0 2480544 2484 ?? Ss 7:12PM 0:00.01 memcached -d telnet操作memcached 连接和退出连接: telnet ip地址 [11211]退出: quit 基本命令 存储命令 基本语法： 12command key flags exptime bytes [noreply] value 参数说明： command: 存储命令，包含set/add/replace/append/prepend key: 键值 key-value 结构中的 key，用于查找缓存值。 flags: 可以包括键值对的整型参数，客户机使用它存储关于键值对的额外信息 。 exptime: 在缓存中保存键值对的时间长度（以秒为单位，0 表示永远）。 bytes: 在缓存中存储的字节数。 noreply(可选): 该参数告知服务器不需要返回数据。 value: 存储的值（始终位于第二行)（可直接理解为key-value结构中的value）。 示例： 12345678910111213141516171819202122jockie:~$ telnet 127.0.0.1 11211Trying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;.set username 0 0 4jackSTOREDadd username 0 0 4johnNOT_STOREDadd pass 0 0 6123456STOREDreplace username 0 0 5peterSTOREDappend username 0 0 5athonSTOREDprepend pass 0 0 200STORED 查找命令 基本语法 1command key1 [key2 key3] 参数说明： command: 包含get/gets/delete 示例： 1234567891011121314get usernameVALUE username 0 9johnathonENDget username passVALUE username 0 9johnathonVALUE pass 0 800123456ENDdelete passDELETEDget passEND 自增自减命令 基本语法： 1command key value 参数说明： command： 包含incr/decr value: 自增或自减的值 示例： 12345678910111213141516171819set height 0 0 3180STOREDget heightVALUE height 0 3180ENDincr height 5185get heightVALUE height 0 3185ENDdecr height 10175get heightVALUE height 0 3175END 统计命令 stats 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081statsSTAT pid 4974STAT uptime 4990STAT time 1512477311STAT version 1.5.3STAT libevent 2.1.8-stableSTAT pointer_size 64STAT rusage_user 0.510169STAT rusage_system 0.355403STAT max_connections 1024STAT curr_connections 10STAT total_connections 20STAT rejected_connections 0STAT connection_structures 11STAT reserved_fds 20STAT cmd_get 27STAT cmd_set 19STAT cmd_flush 6STAT cmd_touch 0STAT get_hits 16STAT get_misses 11STAT get_expired 0STAT get_flushed 4STAT delete_misses 0STAT delete_hits 2STAT incr_misses 0STAT incr_hits 1STAT decr_misses 0STAT decr_hits 1STAT cas_misses 0STAT cas_hits 0STAT cas_badval 0STAT touch_hits 0STAT touch_misses 0STAT auth_cmds 0STAT auth_errors 0STAT bytes_read 1015STAT bytes_written 786STAT limit_maxbytes 67108864STAT accepting_conns 1STAT listen_disabled_num 0STAT time_in_listen_disabled_us 0STAT threads 4STAT conn_yields 0STAT hash_power_level 16STAT hash_bytes 524288STAT hash_is_expanding 0STAT slab_reassign_rescues 0STAT slab_reassign_chunk_rescues 0STAT slab_reassign_evictions_nomem 0STAT slab_reassign_inline_reclaim 0STAT slab_reassign_busy_items 0STAT slab_reassign_busy_deletes 0STAT slab_reassign_running 0STAT slabs_moved 0STAT lru_crawler_running 0STAT lru_crawler_starts 3315STAT lru_maintainer_juggles 8302STAT malloc_fails 0STAT log_worker_dropped 0STAT log_worker_written 0STAT log_watcher_skipped 0STAT log_watcher_sent 0STAT bytes 144STAT curr_items 2STAT total_items 15STAT slab_global_page_pool 0STAT expired_unfetched 0STAT evicted_unfetched 0STAT evicted_active 0STAT evictions 0STAT reclaimed 1STAT crawler_reclaimed 0STAT crawler_items_checked 2STAT lrutail_reflocked 0STAT moves_to_cold 16STAT moves_to_warm 5STAT moves_within_lru 0STAT direct_reclaims 0STAT lru_bumps_dropped 0END stats items 1234567891011121314151617181920212223242526272829stats itemsSTAT items:1:number 2STAT items:1:number_hot 0STAT items:1:number_warm 0STAT items:1:number_cold 2STAT items:1:age_hot 0STAT items:1:age_warm 0STAT items:1:age 705STAT items:1:evicted 0STAT items:1:evicted_nonzero 0STAT items:1:evicted_time 0STAT items:1:outofmemory 0STAT items:1:tailrepairs 0STAT items:1:reclaimed 1STAT items:1:expired_unfetched 0STAT items:1:evicted_unfetched 0STAT items:1:evicted_active 0STAT items:1:crawler_reclaimed 0STAT items:1:crawler_items_checked 2STAT items:1:lrutail_reflocked 0STAT items:1:moves_to_cold 16STAT items:1:moves_to_warm 5STAT items:1:moves_within_lru 0STAT items:1:direct_reclaims 0STAT items:1:hits_to_hot 5STAT items:1:hits_to_warm 0STAT items:1:hits_to_cold 11STAT items:1:hits_to_temp 0END stats slabs 1234567891011121314151617181920tats slabsSTAT 1:chunk_size 96STAT 1:chunks_per_page 10922STAT 1:total_pages 1STAT 1:total_chunks 10922STAT 1:used_chunks 2STAT 1:free_chunks 10920STAT 1:free_chunks_end 0STAT 1:mem_requested 144STAT 1:get_hits 16STAT 1:cmd_set 19STAT 1:delete_hits 2STAT 1:incr_hits 1STAT 1:decr_hits 1STAT 1:cas_hits 0STAT 1:cas_badval 0STAT 1:touch_hits 0STAT active_slabs 1STAT total_malloced 1048576END 清理缓存命令 基本语法： 1flush_all [time] [noreply] 参数说明： [time]可选参数，用于在指定时间执行清理缓存操作。 示例：123456flush_allOKget usernameENDget passEND python操作memcached 安装python-memcached: pip install python-memcached 连接memcached 12import memcachemc = memcache.Client([&apos;127.0.0.1:11211&apos;], debug=True) 设置数据 123mc.set(&apos;username&apos;, &apos;john&apos;, time=60)mc.set(&apos;count&apos;,10,time=60)mc.set_multi(&#123;&apos;gender&apos;: &apos;male&apos;, &apos;phone&apos;: &apos;111111&apos;&#125;, time=60 * 2) 获取数据 1print(mc.get(&apos;phone&apos;)) 删除数据 1mc.delete(&apos;gender&apos;) 自增、自减 12mc.incr(&apos;count&apos;) # delta默认值为1mc.decr(&apos;count&apos;, delta=3)]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python built-in constants]]></title>
    <url>%2Fpython%2Fpython-built-in-constants%2F</url>
    <content type="text"><![CDATA[python内置常量Falsebool类型的假值，python3中，False作为一个关键字不能被赋值，赋值是非法的，会报语法错误： In [1]: False = 0 File &quot;&lt;ipython-input-1-389dfc15b373&gt;&quot;, line 1 False = 0 ^ SyntaxError: can&apos;t assign to keyword True同False一样，bool类型的真值，python3中，True作为一个关键字不能被赋值，赋值是非法的，会报语法错误： In [1]: True = 1 File &quot;&lt;ipython-input-2-11e39cdf8368&gt;&quot;, line 1 True = 1 ^ SyntaxError: can&apos;t assign to keyword NoneNoneType类型的唯一返回值，None通常被用来表示一个值得缺失，同样也不能被赋值 In [1]: None = [] File &quot;&lt;ipython-input-4-263c1fbabdb8&gt;&quot;, line 1 None = [] ^ SyntaxError: can&apos;t assign to keyword NotImplementedEllipsis__debug__如果不是以-o运行python，返回True quit(code=Node)exit(code=None)copyrightIn [1]: copyright Out[1]: Copyright (c) 2001-2016 Python Software Foundation. All Rights Reserved. Copyright (c) 2000 BeOpen.com. All Rights Reserved. Copyright (c) 1995-2001 Corporation for National Research Initiatives. All Rights Reserved. Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam. All Rights Reserved. licenseIn [1]: license Out[1]: Type license() to see the full license text In [2]: license() A. HISTORY OF THE SOFTWARE ========================== Python was created in the early 1990s by Guido van Rossum at Stichting Mathematisch Centrum (CWI, see http://www.cwi.nl) in the Netherlands as a successor of a language called ABC. Guido remains Python&apos;s principal author, although it includes many contributions from others. In 1995, Guido continued his work on Python at the Corporation for National Research Initiatives (CNRI, see http://www.cnri.reston.va.us) in Reston, Virginia where he released several versions of the software. In May 2000, Guido and the Python core development team moved to BeOpen.com to form the BeOpen PythonLabs team. In October of the same year, the PythonLabs team moved to Digital Creations (now Zope Corporation, see http://www.zope.com). In 2001, the Python Software Foundation (PSF, see http://www.python.org/psf/) was formed, a non-profit organization created specifically to own Python-related Intellectual Property. Zope Corporation is a sponsoring member of the PSF. All Python releases are Open Source (see http://www.opensource.org for Hit Return for more, or q (and Return) to quit: creditsIn [1]: credits Out[1]: Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands for supporting Python development. See www.python.org for more information.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>built-in constants</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python built-in functions]]></title>
    <url>%2Fpython%2Fpython-built-in-functions%2F</url>
    <content type="text"><![CDATA[python内置函数python解释器内置许多函数和类型，我们可以直接使用，下表按字典顺序列出 内置函数 abs() dict() help() min() setattr() all() dir() hex() next() slice() any() divmod() id() object() sorted() ascii() enumerate() input() oct() staticmethod() bin() eval() int() open() str() bool() exec() isinstance() ord() sum() bytearray() filter() issubclass() pow() super() bytes() float() iter() print() tuple() callable() format() len() property() type() chr() frozenset() list() range() vars() classmethod() getattr() locals() repr() zip() compile() globals() map() reversed() __import__() complex() hasattr() max() round() delattr() hash() memoryview() set() abs(x)返回一个数的绝对值。参数可以是一个整型或者浮点型。如果参数是一个复数的话，则返回magnitue magnitude的计算规则如下： 实数的magnitude就是该实数的正平方根，如2的magnitude就是2，-3的magnitude就是3 复数的magnitude是该复数与共轭复数的乘积的正平方根，比如z=3-2j，则magnitude为(3-2j)*(3+2j)的正平方根，也就是9+4＝13的正平方根；示例： In [1]: abs(7) Out[1]: 7 In [2]: abs(-7.7) Out[2]: 7.7 In [3]: abs(complex(7,7)) Out[3]: 9.899494936611665 all(iterable)参数为可迭代对象，如果对象的所有元素为真或者对象为空，则返回True，等价于： def all(iterable): for element in iterable: if not element: return False return True 示例： In [6]: all([1,2,3]) Out[6]: True In [7]: all(()) Out[7]: True In [10]: all([1,2,3,None]) Out[10]: False any(iterable)参数为可迭代对象，如果对象的任一元素为真，则返回True，如果对象为空，则返回False，等价于： def any(iterable): for element in iterable: if element: return True return False 示例： In [11]: any([True,False]) Out[11]: True In [12]: any({}) Out[12]: False asciibinboolbytearraybytescallablechrclassmethodcompilecomplexdelattrdictdirdivmodenumerateevalexecfilterfloatformatfrozensetgetattrglobalshasattrhashhelphexid(object)返回一个对象的‘标识’,在一个对象的生命周期中，保持唯一不变。两个不处在同一生命周期的对象调用id()也许会返回一样的值 CPython实现的细节：返回值是对象在内存中的地址 &gt;&gt;&gt; s1 = &apos;sss&apos; &gt;&gt;&gt; s2 = &apos;sss&apos; &gt;&gt;&gt; s3 = &apos;s*s&apos; &gt;&gt;&gt; s4 = &apos;s*s&apos; &gt;&gt;&gt; id(s1) 4351970960 &gt;&gt;&gt; id(s2) 4351970960 &gt;&gt;&gt; id(s3) 4351970640 &gt;&gt;&gt; id(s4) 4351970680 inputintisinstanceissubclassiterlenlistlocalsmaxabsmemoryviewminnextobjectoctordpowprint(*objects, sep=&#39; &#39;, end=&#39;\n&#39;, file=sys.stdout, flush=False)propertyrangereprreversedroundsetsetattrslicesortedstaticmethodstrsumsupertupletypevarszip__import__]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>built-in functions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install mysql5.7 on aliyun ECS ubuntu16.04.4]]></title>
    <url>%2Fdatabase%2FInstall-mysql5-7-on-aliyun-ECS-ubuntu16-04-4%2F</url>
    <content type="text"><![CDATA[阿里云ECS(ubuntu16.04.4)安装MySQL并设置远程访问(以下以root用户操作)更新源apt-get update 安装mysql服务器apt-get install mysql-server apt-get install mysql-client apt-get install libmysqlclient-dev 查看是否安装成功(安装之前也可以使用此命令查看是否有安装MySQL)netstat -tap | grep mysql 重启服务/etc/init.d/mysql restart 登录(假设用户名和密码都为root)mysql -uroot -proot 正常情况按照以上操作，应该可以进入mysql了。 远程访问MySQL，阿里云默认只能本地连接，需要以下操作支持远程访问数据库安装后 查看数据库 mysql&gt; show databases; +——————–+ | Database | +——————–+ | information_schema | | mysql | | performance_schema | | sys | +——————–+ 4 rows in set (0.00 sec) 切换到mysql库 mysql&gt; use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed 查看host,user mysql&gt; select distinct host,user from user; +———–+——————+ | host | user | +———–+——————+ | localhost | root | | localhost | debian-sys-maint | | localhost | mysql.session | | localhost | mysql.sys | +———–+——————+ 4 rows in set (0.01 sec) 更改host以支持远程访问 mysql&gt; update user set host=’%’ where user=’root’ and host=’localhost’; 刷新权限，使配置生效 mysql&gt; flush privileges; 一般情况下，到这里就可以远程访问了，但是由于MySQL是安装在阿里云上的，阿里云默认没有开启3306端口，所以需要到阿里云控制台安全组规则配置规则，开启3306端口，默认情况下只有后五条，开启后会多出第一条，如下图]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>aliyun</tag>
        <tag>ubuntu</tag>
        <tag>ECS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello flask 02: hello world]]></title>
    <url>%2Fpython%2Fhello-flask-02-hello-world%2F</url>
    <content type="text"><![CDATA[flask系列之Hello World!上一篇文章中，已经搭建好了基本环境，这一篇正式进入flask!安装flaskpip install flask 查看是否安装成功，pip freeze显示如下说明安装成功，同时也可以看出flask依赖如下一些包：click,itsdangerous,jinja2,MarkupSafe,Werkzeug,后面会有相关的一些介绍，这里知道就好(micro-blog) jockie:~/programs/learn_flask/flask_microblog (master *)$ pip freeze certifi==2017.7.27.1 click==6.7 Flask==0.12.2 itsdangerous==0.24 Jinja2==2.9.6 MarkupSafe==1.0 Werkzeug==0.12.2 实现flask的hello world应用，这里我们在官方示例的基础上稍微修改一下 创建主入口文件app.py # coding:utf-8 # 导入Flask类和config配置 from flask import Flask from config import config # 实例化一个Flask类 app = Flask(__name__) # 从文件config导入配置，当然在配置项少的情况下也可以直接操作：app.config[&apos;DEBUG&apos;] app.config.from_object(config) # 设置 URL=&apos;/&apos;时的路由规则；以默认GET方法请求访问http://&lt;host:port&gt;/时，调用index(),并将return结果返回给浏览器 @app.route(&apos;/&apos;) def index(): return &apos;&lt;h1&gt;Hello Flask!&lt;/h1&gt;&apos; if __name__ == &apos;__main__&apos;: # 应用的入口函数 app.run() 在app.py中可以看到除了import Flask之外，还import了config文件，config的做用是存放开发所有的一些配置文件，比如在下面创建的config.py中的DEBUG配置，以及后面涉及到的数据库配置等 # coding:utf-8 # 开启DEBUG模式，便于开发测试，生产环境不建议开启 DEBUG = True 运行入口文件app.py文件,程序运行在默认5000端口 (micro-blog) jockie:~/programs/learn_flask/flask_microblog (master *)$ python app.py * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 210-650-536 进入浏览器，查看结果如下]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello flask 01: prepare]]></title>
    <url>%2Fpython%2Fhello-flask-01-prepare%2F</url>
    <content type="text"><![CDATA[flask系列之environment代码库托管github上，在GitHub上创建一个repository(默认已经安装git)： 仓库地址：https://github.com/keepwonder/flask-microblog.git 创建本地开发目录flask_microblog并和GitHub关联 cd /Users/jockie/programs/learn_flask/flask_microblog # 进入项目根目录 git init # 初始化git仓库 echo “# Micro Blog with Python Flask” &gt;&gt; README.md # 创建README文件 git add . # 将新建的README添加进git缓冲 git commit -m ‘first commit’ # 提交文件至git仓库 git remote add origin https://github.com/keepwonder/flask-microblog.git # 添加远程git仓库 git push -u origin master # 将文件推送至远程git仓库master分支 至此，本地目录已与GitHub仓库关联，以后每次修改本地代码都可以同步推送至GitHub，通过GitHub达到管理跟踪代码的目的 本地采用虚拟环境隔离这个项目环境，使用conda管理： 新建一个conda环境 conda create –name micro-blog python=2.7 激活micro-blog环境,目录前会显示当前所在环境(micro-blog),以后所有操作，都基于此环境 source activate micro-blog (micro-blog) jockie:~/programs/learn_flask/flask_microblog (master)$ 在当前环境，运行pip命令，查看相关环境信息,python版本为2.7.14，pip freeze只安装了一个包 (micro-blog) jockie:~$ pip list Package Version ---------- ----------------- certifi 2017.7.27.1 pip 9.0.1 setuptools 36.5.0.post20170921 wheel 0.29.0 (micro-blog) jockie:~$ pip freeze certifi==2017.7.27.1 (micro-blog) jockie:~$ python –version Python 2.7.14 :: Anaconda, Inc. 退出当前虚拟环境 source deactivate 至此，基本环境已经搭建完成，下一步就正式开启项目！]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conda cheet sheet]]></title>
    <url>%2Fpython%2Fconda-cheet-sheet%2F</url>
    <content type="text"><![CDATA[Conda basics: 描述 命令 查看conda是否安装，版本信息 conda info 更新conda至最新版本 conda update conda 安装Anaconda内置的包 conda install PACKAGENAME 运行安装过的包，例如Spyder spyder 更新安装过的程序 conda update PACKAGENAME 获取命令行帮助 COMMANDNAME –help(help前面两个-)例如：conda install –help Using enviroments: 描述 命令 创建一个环境，命名为py35，并使用python3.5 conda create –name py35 python=3.5 激活并使用新环境 WINDOWS: activate py35LINUX,MacOS: source py35 获取所有安装环境列表，当前激活环境以*标记 conda env listconda info –envs 完全复制一个环境 conda create –clone py35 –name py35-2 列出当前激活环境的所有包和版本信息 conda list 列出版本改变历史信息 conda list –revisions 将环境恢复至之前的版本 conda install –revision 将环境信息保存至文本文件 conda list –explicit &gt; bio-env.txt 完全删除一个环境 conda env remove –name bio-envconda remove –name bio-env –all 退出当前激活环境 WINDOWS: deactivemacOS, LINUX: source deactive 从文本文件创建一个环境 conda env create –file bio-env.txt 栈命令：创建一个新环境，命名为bio-env，再安装biopython包 conda create –name bio-env biopython Finding conda packages 描述 命令 使用conda查找一个包 conda searche PACKAGENAME 查看Anaconda中的所有包 https://docs.anaconda.com/anaconda/packages/pkg-docs Installing and updating packages 描述 命令 安装一个新包(Jupyter Notebook) conda install jupter 运行安装过的包(Jupyter-notebook) jupyter-notebook 在非但前激活环境(bio-env)中安装一个新包(toolz) conda install –name bio-env toolz 更新当前激活环境中的一个包 conda update scikit-learn 从指定的渠道(conda-forge)安装一个包(boltons) conda install –channel conda-forge boltons 在当前激活环境使用pip直接从PyPI安装一个包 pip install boltons 从指定环境(bio-env)删除一个或多个包(toolz,bltons) conda remove –name bio-env toolz boltons Managing multiple versions of Python 描述 命令 在一个新的名为py34的环境中安装一个不同版本的python conda create –name py34 python=3.4 切换至拥有不同python版本的环境中 WINDOWS: activate py34LINUX,MacOS: source activate 显示当前python所在的路径 WINDOWS: where pythonLINUX,MacOS: which python 显示当前python版本信息 python –version Specify version numbers使用conda create,conda install命令或者在meta.yaml文件中指定包的版本的方法 Constraint type Specification Result Fuzzy numpy=1.11 1.11.0, 1.11.1, 1.11.2, 1.11.18 etc. Exact numpy==1.11 1.11.0 Greater than or equal to “numpy&gt;=1.11” 1.11.0 or hihger OR “numpy=1.11.1&#124;1.11.3” 1.11.1, 1.11.3 AND “numpy&gt;=1.8,&lt;2” 1.8, 1.9, not 2.0 notes:markdown表格内换行:&lt;br&gt;stack commands:感觉只可意会，翻译成堆栈命令有点奇怪markdown表格转义|:使用ASCII字符集&amp;#124;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[problems in using python]]></title>
    <url>%2Fpython%2Fproblems-in-using-python%2F</url>
    <content type="text"><![CDATA[(声明，以下Q代表问题，A代表答案) Q1: 安装mysql-python库报错，如下： 描述 (python27) jockie:/usr/local/bin$ pip install mysql-python Collecting mysql-python Using cached MySQL-python-1.2.5.zip Complete output from command python setup.py egg_info: sh: mysql_config: command not found Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;/private/var/folders/yf/m8nhcn4x1vd1snmnykt7n89h0000gn/T/pip-build-sSaKtK/mysql-python/setup.py&quot;, line 17, in &lt;module&gt; metadata, options = get_config() File &quot;setup_posix.py&quot;, line 43, in get_config libs = mysql_config(&quot;libs_r&quot;) File &quot;setup_posix.py&quot;, line 25, in mysql_config raise EnvironmentError(&quot;%s not found&quot; % (mysql_config.path,)) EnvironmentError: mysql_config not found ---------------------------------------- Command &quot;python setup.py egg_info&quot; failed with error code 1 in /private/var/folders/yf/m8nhcn4x1vd1snmnykt7n89h0000gn/T/pip-build-sSaKtK/mysql-python/ A1: 找不到mysql_config，原因mysql安装目录/usr/local/mysql/bin不在PATH中,临时解决方法： export PATH=$PATH:/usr/local/mysql/bin Q2: import循环引用，导致ImportError， 描述：如下两个文件互相调用： #home/__init__.py from flask import Blueprint import app.home.views home = Blueprint(&apos;home&apos;, __name__) #home/views.py from . import home @home.route(&apos;/&apos;) def index(): return &apos;&lt;h1 style=&quot;color:green&quot;&gt;this is home!&lt;/h1&gt;&apos; Traceback (most recent call last): File &quot;/Users/jockie/programs/micro_movie/manage.py&quot;, line 7, in &lt;module&gt; from app import app File &quot;/Users/jockie/programs/micro_movie/app/__init__.py&quot;, line 8, in &lt;module&gt; from app.home import home as home_blueprint File &quot;/Users/jockie/programs/micro_movie/app/home/__init__.py&quot;, line 9, in &lt;module&gt; import app.home.views File &quot;/Users/jockie/programs/micro_movie/app/home/views.py&quot;, line 7, in &lt;module&gt; from . import home ImportError: cannot import name &apos;home&apos; A2: 程序执行的顺序是从上往下执行，所以要先定义home对象，再导入视图！修改init.py中import顺序 #home/__init__.py from flask import Blueprint home = Blueprint(&apos;home&apos;, __name__) import app.home.views Q3: flask程序中，修改css后，浏览器无法实时显示更新？ A3: 需要清除浏览器缓存(设置debug=True无效) Q4: UnicodeEncodeError: ‘ascii’ codec can’t encode character ‘\u3000’ in position 135: ordinal not in range(128)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7+hadoop2.8.0 full cluster]]></title>
    <url>%2FBigData%2Fcentos7-hadoop2-8-0-full-cluster%2F</url>
    <content type="text"><![CDATA[hadoop完全分布式安装环境 主机：3台centos7主机，用户名和密码都设置为root/root，hd/hd hadoop版本：hadoop2.8.0 jdk版本：java1.8.0 配置节点 设置主机ip 主机 IP 对应角色 master 192.168.140.135 namendoe slave1 192.168.140.136 datanode slave2 192.168.140.137 datanode 永久修改hostname 分别设为 hostnamectl set-hostname master hostnamectl set-hostname slave1 hostnamectl set-hostname slave2 设置 /etc/hosts文件 ，添加如下 192.168.140.135 master 192.168.140.136 slave1 192.168.140.137 slave2 查看java版本,确保java已经安装[hd@master bigdata]$ java -version java version &quot;1.8.0_131&quot; Java(TM) SE Runtime Environment (build 1.8.0_131-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) [hd@master bigdata]$ 关闭防火墙，关闭firewall 查看防火墙状态 [root@master ~]# firewall-cmd –state running 停止服务 [root@master ~]# systemctl stop firewalld.service [root@master ~]# firewall-cmd –state not running 禁用防火墙 [root@master ~]# systemctl disable firewalld.service Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service. [root@master ~]# firewall-cmd –state not running hd作为大数据环境用户，给hd用户增加sudo权限[root@master ~]# ll /etc/sudoers -r-xr-----. 1 root root 3907 Nov 4 2016 /etc/sudoers [root@master ~]# chmod u+w /etc/sudoers [root@master ~]# vi /etc/sudoers ## Allow root to run any commands anywhere root ALL=(ALL) ALL hd ALL=(ALL)NOPASSWD:ALL [root@master ~]# chmod u-w /etc/sudoers 切换到hd用户下,配置SSH免密码登录 ssh-keygen -t rsa -P &apos;&apos; cp id_rsa.pub authorized_keys 配置jdk[hd@master bigdata]$ sudo vi /etc/profile export JAVA_HOME=/usr/local/bigdata/jdk1.8 export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib/ export PATH=$PATH:$JAVA_HOME/bin 配置Hadoop 编辑hadoop-env.sh #The java implementation to use. export JAVA_HOME=/usr/local/bigdata/jdk1.8 编辑core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/hd/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 编辑hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/hd/hadoop/dfs/name&lt;/value&gt; &lt;description&gt;Path on the local filesystem where theNameNode stores the namespace and transactions logs persistently.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/hd/hadoop/dfs/data&lt;/value&gt; &lt;description&gt;Comma separated list of paths on the localfilesystem of a DataNode where it should store its blocks.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;need not permissions&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 编辑mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;master:49001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.local.dir&lt;/name&gt; &lt;value&gt;/hd/hadoop/var&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改hadoop/etc/hadoop/slaves文件，修改如下 slave1 slave2 编辑yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;The address of the applications manager interface in the RM.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;${yarn.resourcemanager.hostname}:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;The address of the scheduler interface.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;${yarn.resourcemanager.hostname}:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;The http address of the RM web application.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;${yarn.resourcemanager.hostname}:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;The https adddress of the RM web application.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address&lt;/name&gt; &lt;value&gt;${yarn.resourcemanager.hostname}:8090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;${yarn.resourcemanager.hostname}:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;The address of the RM admin interface.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;${yarn.resourcemanager.hostname}:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;每个节点可用内存,单位MB,默认8182MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS install scrapy]]></title>
    <url>%2Fpython%2FMacOS-install-scrapy%2F</url>
    <content type="text"><![CDATA[环境系统版本：macOS Sierra 10.12.6python版本：Python3 安装步骤 安装homebrew /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装wget brew install wget 安装command line tools xcode-select --install 安装pip wget https://bootstrap.pypa.io/get-pip.py python get-pip.py 重启系统 重新启动Mac OS 重启时按住Command+R，进入Recovery模式 在Recovery模式中输入: csrutil disable; 重新启动，回到Mac OS 安装scrapy sudo -H pip install Scrapy 查看版本信息 scarpy version]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 install mysql5.7]]></title>
    <url>%2FBigData%2Fcentos7-install-mysql5-7%2F</url>
    <content type="text"><![CDATA[卸载系统自带的Mariadb[root@localhost ~]# rpm -qa | grep mariadb mariadb-libs-5.5.52-1.el7.x86_64 [root@localhost ~]# rpm -e --nodeps mariadb-libs-5.5.52-1.el7.x86_64 [root@localhost ~]# rpm -qa | grep mariadb [root@localhost ~]# 删除etc目录下的my.cnf文件[root@localhost ~]# rm /etc/my.cnf rm: cannot remove ‘/etc/my.cnf’: No such file or directory [root@localhost ~]# 检查mysql是否存在[root@localhost ~]# rpm -qa | grep mysql [root@localhost ~]# 检查mysql组和用户是否存在，如无创建[root@localhost ~]# cat /etc/group | grep mysql [root@localhost ~]# cat /etc/passwd | grep mysql [root@localhost ~]# 创建mysql用户组[root@localhost ~]# groupadd mysql [root@localhost ~]# cat /etc/group | grep mysql mysql:x:1001: 创建一个用户名为mysql的用户并加入mysql用户组[root@localhost ~]# useradd -g mysql mysql [root@localhost ~]# cat /etc/passwd | grep mysql mysql:x:1001:1001::/home/mysql:/bin/bash [root@localhost ~]# 指定password 为 mysql[root@localhost ~]# passwd mysql Changing password for user mysql. New password: BAD PASSWORD: The password is shorter than 8 characters Retype new password: passwd: all authentication tokens updated successfully. [root@localhost ~]# 将mysql安装到/usr/local/bigdata下[root@localhost bigdata]# ll total 625636 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 -rw-r--r--. 1 root root 640650826 Aug 27 05:55 mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost bigdata]# tar -zxf mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost bigdata]# ll total 625636 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 root root 129 Aug 27 05:57 mysql-5.7.19-linux-glibc2.12-x86_64 -rw-r--r--. 1 root root 640650826 Aug 27 05:55 mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost bigdata]# mv mysql-5.7.19-linux-glibc2.12-x86_64 mysql57 [root@localhost bigdata]# ll total 625636 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 root root 129 Aug 27 05:57 mysql57 -rw-r--r--. 1 root root 640650826 Aug 27 05:55 mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost] ## 更改所属的组和用户 [root@localhost bigdata]# chown -R mysql mysql57/ [root@localhost bigdata]# ll total 0 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 mysql root 129 Aug 27 05:57 mysql57 [root@localhost bigdata]# chgrp -R mysql mysql57/ [root@localhost bigdata]# ll total 0 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 mysql mysql 129 Aug 27 05:57 mysql57 root@localhost mysql57]# mkdir data [root@localhost mysql57]# chown -R mysql:mysql data [root@localhost mysql57]# ll total 36 drwxr-xr-x. 2 mysql mysql 4096 Aug 27 05:57 bin -rw-r--r--. 1 mysql mysql 17987 Jun 22 10:13 COPYING drwxr-xr-x. 2 mysql mysql 6 Aug 27 06:02 data drwxr-xr-x. 2 mysql mysql 55 Aug 27 05:57 docs drwxr-xr-x. 3 mysql mysql 4096 Aug 27 05:57 include drwxr-xr-x. 5 mysql mysql 229 Aug 27 05:57 lib drwxr-xr-x. 4 mysql mysql 30 Aug 27 05:57 man -rw-r--r--. 1 mysql mysql 2478 Jun 22 10:13 README drwxr-xr-x. 28 mysql mysql 4096 Aug 27 05:57 share drwxr-xr-x. 2 mysql mysql 90 Aug 27 05:57 support-files [root@localhost mysql57]# 在etc下新建配置文件my.cnf，并在该文件内添加以下配置[mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 [mysqld] skip-name-resolve #设置3306端口 port = 3306 # 设置mysql的安装目录 basedir=/usr/local/bigdata/mysql57 # 设置mysql数据库的数据的存放目录 datadir=/usr/local/bigdata/mysql57/data # 允许最大连接数 max_connections=200 # 服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB lower_case_table_names=1 max_allowed_packet=16M 安装和初始化[root@localhost mysql57]# bin/mysql_install_db --user=mysql --basedir=/usr/local/bigdata/mysql57 --datadir=/usr/local/bigdata/mysql57/data/ 2017-08-27 06:08:18 [WARNING] mysql_install_db is deprecated. Please consider switching to mysqld --initialize 2017-08-27 06:08:20 [WARNING] The bootstrap log isn&apos;t empty: 2017-08-27 06:08:20 [WARNING] 2017-08-27T10:08:18.703344Z 0 [Warning] --bootstrap is deprecated. Please consider using --initialize instead 2017-08-27T10:08:18.705509Z 0 [Warning] Changed limits: max_open_files: 1024 (requested 5000) 2017-08-27T10:08:18.705523Z 0 [Warning] Changed limits: table_open_cache: 407 (requested 2000) [root@localhost mysql57]# [root@localhost mysql57]# cp ./support-files/mysql.server /etc/init.d/mysqld [root@localhost mysql57]# chown 777 /etc/my.cnf [root@localhost mysql57]# chmod +x /etc/init.d/mysqld [root@localhost mysql57]# /etc/init.d/mysqld restart ERROR! MySQL server PID file could not be found! Starting MySQL.Logging to &apos;/usr/local/bigdata/mysql57/data/localhost.localdomain.err&apos;. SUCCESS! 设置开机启动 [root@localhost mysql57]# chkconfig --level 35 mysqld on [root@localhost mysql57]# chkconfig --list mysqld Note: This output shows SysV services only and does not include native systemd services. SysV configuration data might be overridden by native systemd configuration. If you want to list systemd services use &apos;systemctl list-unit-files&apos;. To see services enabled on particular target use &apos;systemctl list-dependencies [target]&apos;. mysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off [root@localhost mysql57]# chmod +x /etc/rc.d/init.d/mysqld [root@localhost mysql57]# chkconfig --add mysqld [root@localhost mysql57]# chkconfig --list mysqld Note: This output shows SysV services only and does not include native systemd services. SysV configuration data might be overridden by native systemd configuration. If you want to list systemd services use &apos;systemctl list-unit-files&apos;. To see services enabled on particular target use &apos;systemctl list-dependencies [target]&apos;. mysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off [root@localhost mysql57]# service mysqld status SUCCESS! MySQL running (4822) [root@localhost mysql57]# 设置/etc/profile/export PATH=$PATH:/usr/local/bigdata/mysql557/bin 获得初始密码[root@localhost mysql57]# cat /root/.mysql_secret # Password set for user &apos;root@localhost&apos; at 2017-08-27 06:08:18 zm&amp;nSHM5Etpw 修改密码root@localhost mysql57]# mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 4 Server version: 5.7.19 Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; set PASSWORD = PASSWORD(&apos;root&apos;) -&gt; ; Query OK, 0 rows affected, 1 warning (0.01 sec) mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec) mysql&gt; 添加远程访问权限mysql&gt; use mysql Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; update user set host=&apos;%&apos; where user=&apos;root&apos;; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql&gt; select host,user from user; +-----------+---------------+ | host | user | +-----------+---------------+ | % | root | | localhost | mysql.session | | localhost | mysql.sys | +-----------+---------------+ 3 rows in set (0.00 sec) 重启生效[root@localhost mysql57]# systemctl restart mysql.service [root@localhost mysql57]# /etc/init.d/mysqld restart Shutting down MySQL.. SUCCESS! Starting MySQL. SUCCESS! 为了在任何目录下可以登录mysql[root@localhost mysql57]# ln -s /usr/local/bigdata/mysql57/bin/mysql /usr/bin/mysql]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>mysql5.7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install and Configure HBase]]></title>
    <url>%2FBigData%2FUbuntu16-04-Install-and-Configure-HBase%2F</url>
    <content type="text"><![CDATA[HBase安装 下载安装包从HBase官网下载最新稳定版，目前为hbase-1.2.6-bin.tar.gz 解压安装包将安装包解压至/usr/local目录下，并重命名为hbase,并给hbase目录用户和所属组都改为hadoop hadoop@ubuntu16:/usr/local$ sudo chown -R hadoop:hadoop hbase hadoop@ubuntu16:/usr/local$ ll | grep hbase drwxrwxr-x 9 hadoop hadoop 4096 8月 19 19:59 hbase/ hadoop@ubuntu16:/usr/local$ cd hbase/ hadoop@ubuntu16:/usr/local/hbase$ ll total 356 drwxrwxr-x 9 hadoop hadoop 4096 8月 19 19:59 ./ drwxr-xr-x 15 root root 4096 8月 19 19:42 ../ drwxr-xr-x 4 hadoop hadoop 4096 1月 29 2016 bin/ -rw-r--r-- 1 hadoop hadoop 129552 5月 29 14:29 CHANGES.txt drwxr-xr-x 2 hadoop hadoop 4096 8月 19 19:58 conf/ drwxr-xr-x 12 hadoop hadoop 4096 5月 29 15:20 docs/ drwxrwxr-x 7 hadoop hadoop 4096 8月 19 19:59 hbase-tmp/ drwxr-xr-x 7 hadoop hadoop 4096 5月 29 14:53 hbase-webapps/ -rw-rw-r-- 1 hadoop hadoop 261 5月 29 15:31 LEGAL drwxrwxr-x 3 hadoop hadoop 4096 8月 19 19:41 lib/ -rw-rw-r-- 1 hadoop hadoop 143082 5月 29 15:31 LICENSE.txt drwxrwxr-x 2 hadoop hadoop 4096 8月 19 19:59 logs/ -rw-rw-r-- 1 hadoop hadoop 42115 5月 29 15:31 NOTICE.txt -rw-r--r-- 1 hadoop hadoop 1477 12月 27 2015 README.txt 配置安装路径修改hadoop用户下.bashrc文件，添加如下: export HBASE_HOME=/usr/local/hbase export PATH=$PATH:$HBASE_HOME/bin:$PATH 执行source命令使修改生效: hadoop@ubuntu16:~$ source .bashrc 验证安装是否成功 hadoop@ubuntu16:~$ hbase version HBase 1.2.6 Source code repository file:///home/busbey/projects/hbase/hbase-assembly/target/hbase-1.2.6 revision=Unknown Compiled by busbey on Mon May 29 02:25:32 CDT 2017 From source with checksum 7e8ce83a648e252758e9dae1fbe779c9 看到以上消息表示hbase成功安装。 HBase单机模式 配置/conf/hbase-env.sh配置JAVA_HOME和HBASE_MANAGES_ZK两个变量 export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export HBASE_MANAGES_ZK=true 配置/conf/hbase-site.xml在启动Hbase前需要设置属性hbase.rootdir，用于指定Hbase数据的存储位置，此处设置为HBase安装目录下的hbase-tmp文件夹即（file:///usr/local/hbase/hbase-tmp），配置如下： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///usr/local/hbase/hbase-tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动HBase 启动之前先jps查看下，这里hadoop已经运行，hbase单机模式可以不开启hadoop hadoop@ubuntu16:/usr/local/hbase/conf$ jps 16913 RunJar 14580 NodeManager 18182 Jps 14935 JobHistoryServer 14105 DataNode 14459 ResourceManager 14302 SecondaryNameNode 13951 NameNode 使用start-hbase.sh脚本启动hbase hadoop@ubuntu16:/usr/local/hbase/conf$ start-hbase.sh starting master, logging to /usr/local/hbase/logs/hbase-hadoop-master-ubuntu16.out 再jps查看下,多出个HMaster进程 hadoop@ubuntu16:/usr/local/hbase/conf$ jps 16913 RunJar 18307 HMaster 14580 NodeManager 14935 JobHistoryServer 18615 Jps 14105 DataNode 14459 ResourceManager 14302 SecondaryNameNode 13951 NameNode hadoop@ubuntu16:/usr/local/hbase/conf$ jps | grep HM 18307 HMaster 进入hbase shell 进入shell模式之后，通过status命令查看运行状态，通过exit退出shell hadoop@ubuntu16:/usr/local/hbase/conf$ hbase shell SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands. Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017 hbase(main):001:0&gt; status 1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load hbase(main):002:0&gt; exit hadoop@ubuntu16:/usr/local/hbase/conf$ 停止HBase hadoop@ubuntu16:/usr/local/hbase/conf$ stop-hbase.sh stopping hbase................. hadoop@ubuntu16:/usr/local/hbase/conf$ HBase伪分布式 配置/conf/hbase-env.sh添加如下 export HBASE_CLASSPATH=/usr/local/hadoop/conf 配置/conf/hbase-site.xml修改hbase.rootdir，将其指向localhost(与hdfs的端口保持一致)，并指定HBase在HDFS上的存储路径。将属性hbase.cluter.distributed设置为true。假设当前Hadoop集群运行在伪分布式模式下，且NameNode运行在9000端口； &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动HBase 启动之前，保证hadoop已经启动，可用jps查看进程 hadoop@ubuntu16:/usr/local/hbase/conf$ start-hbase.sh localhost: starting zookeeper, logging to /usr/local/hbase/bin/../logs/hbase-hadoop-zookeeper-ubuntu16.out starting master, logging to /usr/local/hbase/logs/hbase-hadoop-master-ubuntu16.out starting regionserver, logging to /usr/local/hbase/logs/hbase-hadoop-1-regionserver-ubuntu16.out jps查看进程 hadoop@ubuntu16:/usr/local/hbase/conf$ jps 16913 RunJar 14580 NodeManager 23094 HQuorumPeer 14935 JobHistoryServer 23287 HRegionServer 23160 HMaster 23880 Jps 14105 DataNode 14459 ResourceManager 14302 SecondaryNameNode 13951 NameNode 再过滤下，可看出多出如下3个进程 hadoop@ubuntu16:/usr/local/hbase/conf$ jps | awk &apos;{print $2}&apos;| grep &apos;^H&apos; HQuorumPeer HRegionServer HMaster 进程shell模式进入shell模式之后，通过list命令查看当前数据库所有表信息，通过create命令创建一个employee表，其拥有employee_id,address,info三个列族，通过describe命令查看employee表结构，通过exit命令退出HBase shell模式。 hadoop@ubuntu16:/usr/local/hbase/conf$ hbase shell SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands. Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017 hbase(main):001:0&gt; create &apos;employee&apos;,&apos;employee_id&apos;,&apos;address&apos;,&apos;info&apos; 0 row(s) in 1.8040 seconds =&gt; Hbase::Table - employee hbase(main):006:0&gt; list TABLE employee 1 row(s) in 0.0080 seconds =&gt; [&quot;employee&quot;] hbase(main):007:0&gt; describe &apos;employee&apos; Table employee is ENABLED employee COLUMN FAMILIES DESCRIPTION {NAME =&gt; &apos;address&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;} {NAME =&gt; &apos;employee_id&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;fal se&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREV ER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;} {NAME =&gt; &apos;info&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KE EP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, CO MPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65 536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;} 3 row(s) in 0.0650 seconds hbase(main):008:0&gt; exit hadoop@ubuntu16:/usr/local/hbase/conf$ 查看HDFS的HBase数据库文件通过hdfs dfs –ls /hbase命令查看HBase分布式数据库在HDFS上是否成功创建，/hbase/data/default/employee文件夹即为上一步我们所建立的employee在HDFS上的存储位置。 hadoop@ubuntu16:/usr/local/hbase/conf$ hdfs dfs -ls /hbase Found 8 items drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/.tmp drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/MasterProcWALs drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:26 /hbase/WALs drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:38 /hbase/archive drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:10 /hbase/data -rw-r--r-- 1 hadoop supergroup 42 2017-08-19 21:10 /hbase/hbase.id -rw-r--r-- 1 hadoop supergroup 7 2017-08-19 21:10 /hbase/hbase.version drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/oldWALs hadoop@ubuntu16:/usr/local/hbase/conf$ hdfs dfs -ls /hbase/data/default Found 1 items drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/data/default/employee 通过界面查看相关信息主机ip:50070查看hdfs信息,如下图主机ip:16010查看hbase相关新，如下图 停止HBase hadoop@ubuntu16:/usr/local/hbase/conf$ stop-hbase.sh stopping hbase................... localhost: stopping zookeeper. hadoop@ubuntu16:/usr/local/hbase/conf$]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whois]]></title>
    <url>%2Funcategorized%2Fpython-whois%2F</url>
    <content type="text"><![CDATA[在爬取一个网站时，又是我们需要查看一个网站的所有者相关信息，这时就可以用到whois这个模块step1: 使用pip install python-whois安装whois模块(/Users/jockie/install_programs/anaconda) jockie:~/programs/pycharm$ pip install python-whois Collecting python-whois Downloading python-whois-0.6.5.tar.gz Collecting future (from python-whois) Downloading future-0.16.0.tar.gz (824kB) 100% |################################| 829kB 137kB/s Building wheels for collected packages: python-whois, future Running setup.py bdist_wheel for python-whois ... done Stored in directory: /Users/jockie/Library/Caches/pip/wheels/37/68/27/819a3f07cbe75200d8cfa74d4517fd0f402b6dd7aaf91afe8b Running setup.py bdist_wheel for future ... done Stored in directory: /Users/jockie/Library/Caches/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a Successfully built python-whois future Installing collected packages: future, python-whois Successfully installed future-0.16.0 python-whois-0.6.5 step2: 使用whoisIn [1]: import whois In [2]: whois.whois(&apos;xuanxiewu.com&apos;) Out[2]: {&apos;address&apos;: &apos;NanJingShiYuHuaTaiQuXiShanQiaoMeiXinXiaoQu&apos;, &apos;city&apos;: &apos;Nanjing&apos;, &apos;country&apos;: &apos;CN&apos;, &apos;creation_date&apos;: datetime.datetime(2014, 9, 28, 4, 9, 31), &apos;dnssec&apos;: &apos;unsigned&apos;, &apos;domain_name&apos;: [&apos;XUANXIEWU.COM&apos;, &apos;xuanxiewu.com&apos;], &apos;emails&apos;: [&apos;tld@cndns.com&apos;, &apos;domain@cndns.com&apos;, &apos;1044699649@qq.com&apos;], &apos;expiration_date&apos;: [datetime.datetime(2018, 9, 28, 4, 9, 31), datetime.datetime(2018, 9, 28, 12, 7, 1)], &apos;name&apos;: &apos;qiang jiong&apos;, &apos;name_servers&apos;: [&apos;F1G1NS1.DNSPOD.NET&apos;, &apos;F1G1NS2.DNSPOD.NET&apos;, &apos;f1g1ns1.dnspod.net&apos;, &apos;f1g1ns2.dnspod.net&apos;], &apos;org&apos;: &apos;qiang jiong&apos;, &apos;referral_url&apos;: None, &apos;registrar&apos;: &apos;SHANGHAI MEICHENG TECHNOLOGY INFORMATION DEVELOPMENT CO., LTD.&apos;, &apos;state&apos;: &apos;Jiangsu&apos;, &apos;status&apos;: [&apos;clientTransferProhibited https://icann.org/epp#clientTransferProhibited&apos;, &apos;ok https://icann.org/epp#ok&apos;], &apos;updated_date&apos;: [datetime.datetime(2017, 6, 19, 15, 33, 50), datetime.datetime(2017, 6, 19, 23, 33, 51)], &apos;whois_server&apos;: &apos;grs-whois.cndns.com&apos;, &apos;zipcode&apos;: &apos;210041&apos;}]]></content>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use builtwith in python3]]></title>
    <url>%2FDataAnalysis%2Fuse-builtwith-in-python3%2F</url>
    <content type="text"><![CDATA[python3中使用builtwith模块（使用工具pycharm,命令行也是pycharm自带terminal）step1: 使用pip install builtwith 来安装builtwith模块(/Users/jockie/install_programs/anaconda) jockie:~/programs/pycharm$ pip install builtwith Collecting builtwith Downloading builtwith-1.3.2.tar.gz Building wheels for collected packages: builtwith Running setup.py bdist_wheel for builtwith ... done Stored in directory: /Users/jockie/Library/Caches/pip/wheels/e4/cf/86/aa813feb4c79e680590a42766642b130358a01f1e26ecfe1d6 Successfully built builtwith Installing collected packages: builtwith Successfully installed builtwith-1.3.2 step2: 测试builtwith模块import builtwith info = builtwith.parse(&apos;http://www.xuanxiewu.com&apos;) print(info) 运行代码报如下错误 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py Traceback (most recent call last): File &quot;/Users/jockie/programs/pycharm/python_spider/chp01_01.py&quot;, line 8, in &lt;module&gt; import builtwith File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 42 except Exception , e: ^ SyntaxError: invalid syntax Process finished with exit code 1 可以看出报的是语法错误，那为什么会有语法错误呢？原因是builtwith是基于python2.x版本的，所以这里需要做一些相应的语法修改1.python2的‘Exception , e’写法不支持， 修改为Exception as e2.python2的print表达式，修改为print()函数3.builtwith使用的urllib2模块属于python2，python3中使用urllib,所以在init.py源码中使用urllib2的地方都需要改urllib的写法，首先需要将 import urllib2替换成 import urllib.request import urllib.error 再将urllib2相关方法替换 request = urllib.request.Request(url, None, {&apos;User-Agent&apos;: user_agent}) # request = urllib2.Request(url, None, {&apos;User-Agent&apos;: user_agent}) response = urllib.request.urlopen(request) # response = urllib2.urlopen(request) 再次运行代码,报如下错误： /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py Traceback (most recent call last): File &quot;/Users/jockie/programs/pycharm/python_spider/chp01_01.py&quot;, line 10, in &lt;module&gt; info = builtwith.parse(&apos;http://www.baidu.com&apos;) File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 69, in builtwith if contains(html, snippet): File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 111, in contains return re.compile(regex.split(&apos;\\;&apos;)[0], flags=re.IGNORECASE).search(v) TypeError: cannot use a string pattern on a bytes-like object Process finished with exit code 1 可以看出报的是类型错误，这是因为urllib返回的数据格式已经发生了改变，需要进行转码，将下面的代码 if html is None: html = response.read() 改为 if html is None: html = response.read() html = html.decode(&apos;utf-8&apos;) 再次运行代码，得到正确结果 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py {&apos;font-scripts&apos;: [&apos;Font Awesome&apos;, &apos;Google Font API&apos;], &apos;web-frameworks&apos;: [&apos;Twitter Bootstrap&apos;], &apos;javascript-frameworks&apos;: [&apos;jQuery&apos;]} Process finished with exit code 0 但是，再看上面的解码使用的是utf-8，写死了，如果网站用的不是utf-8呢，这里再试验下，以www.163.com为例，使用的是gbk,再次运行，又报如下错误 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py Error: &apos;utf-8&apos; codec can&apos;t decode byte 0xcd in position 565: invalid continuation byte Traceback (most recent call last): File &quot;/Users/jockie/programs/pycharm/python_spider/chp01_01.py&quot;, line 10, in &lt;module&gt; info = builtwith.parse(&apos;http://www.163.com&apos;) File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 69, in builtwith if contains(html, snippet): File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 111, in contains return re.compile(regex.split(&apos;\\;&apos;)[0], flags=re.IGNORECASE).search(v) TypeError: cannot use a string pattern on a bytes-like object Process finished with exit code 1 将编码改为gbk，得到正确结果 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py {&apos;web-servers&apos;: [&apos;Nginx&apos;]} Process finished with exit code 0 那么问题来了，不同的网站编码不一定相同，如果每次换一个网站，就要改一遍编码的话，那将增加许多额外的工作量，也是不现实的，那么有没有方法做到一劳永逸呢，这里就需要引入chardet模块,同样使用：pip install chardet,将builtwith源码，做如下修改 if html is None: html = response.read() # html = html.decode(&apos;utf-8&apos;) # add by Johnahton 20170805 encode_type = chardet.detect(html) if encode_type[&apos;encoding&apos;] == &apos;utf-8&apos;: html = html.decode(&apos;utf-8&apos;) else: html = html.decode(&apos;gbk&apos;) 加入chardet判断字符编码后，就可以一劳永逸了！]]></content>
      <categories>
        <category>DataAnalysis</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install Git]]></title>
    <url>%2Ftools%2FUbuntu16-04-install-git%2F</url>
    <content type="text"><![CDATA[安装Gitubuntu系统安装软件一般直接apt-get install就OK了,所以可以直接用: johnathon@ubuntu16:~$ sudo apt-get install git 查看版本： johnathon@ubuntu16:~$ git --version git version 2.7.4 当前最新版本并不是2.7.4，而是2.13.3，所以直接这样安装的话并不能安装最新的git版本，正确的步骤按照以下进行 sudo add-apt-repository ppa:git-core/ppa johnathon@ubuntu16ohnathon@ubuntu16:~$ sudo add-apt-repository ppa:git-core/ppa The most current stable version of Git for Ubuntu. For release candidates, go to https://launchpad.net/~git-core/+archive/candidate . More info: https://launchpad.net/~git-core/+archive/ubuntu/ppa Press [ENTER] to continue or ctrl-c to cancel adding it gpg: keyring `/tmp/tmpbb9o0lni/secring.gpg&apos; created gpg: keyring `/tmp/tmpbb9o0lni/pubring.gpg&apos; created gpg: requesting key E1DF1F24 from hkp server keyserver.ubuntu.com gpg: /tmp/tmpbb9o0lni/trustdb.gpg: trustdb created gpg: key E1DF1F24: public key &quot;Launchpad PPA for Ubuntu Git Maintainers&quot; imported gpg: Total number processed: 1 gpg: imported: 1 (RSA: 1) OK 更新软件源，sudo apt-get update johnathon@ubuntu16:~$ sudo apt-get update Hit:1 http://cn.archive.ubuntu.com/ubuntu xenial InRelease Hit:2 http://cn.archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:3 http://cn.archive.ubuntu.com/ubuntu xenial-backports InRelease Get:4 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial InRelease [17.5 kB] Hit:5 http://security.ubuntu.com/ubuntu xenial-security InRelease Get:6 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main amd64 Packages [3256 B] Get:7 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main i386 Packages [3248 B] Get:8 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main Translation-en [2496 B] Fetched 26.5 kB in 1s (13.7 kB/s) Reading package lists... Done 安装git，sudo apt-get install git johnathon@ubuntu16:~$ sudo apt-get install git Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: linux-headers-4.8.0-36 linux-headers-4.8.0-36-generic linux-image-4.8.0-36-generic linux-image-extra-4.8.0-36-generic Use &apos;sudo apt autoremove&apos; to remove them. The following additional packages will be installed: git-man Suggested packages: git-daemon-run | git-daemon-sysvinit git-doc git-el git-email git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn The following NEW packages will be installed: git The following packages will be upgraded: git-man 1 upgraded, 1 newly installed, 0 to remove and 129 not upgraded. Need to get 6174 kB of archives. After this operation, 30.4 MB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main amd64 git-man all 1:2.13.0-0ppa1~ubuntu16.04.1 [1448 kB] Get:2 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main amd64 git amd64 1:2.13.0-0ppa1~ubuntu16.04.1 [4726 kB] Fetched 6174 kB in 19s (322 kB/s) perl: warning: Setting locale failed. perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LC_TIME = &quot;zh_CN.UTF-8&quot;, LC_MONETARY = &quot;zh_CN.UTF-8&quot;, LC_CTYPE = &quot;UTF-8&quot;, LC_ADDRESS = &quot;zh_CN.UTF-8&quot;, LC_TELEPHONE = &quot;zh_CN.UTF-8&quot;, LC_NAME = &quot;zh_CN.UTF-8&quot;, LC_MEASUREMENT = &quot;zh_CN.UTF-8&quot;, LC_IDENTIFICATION = &quot;zh_CN.UTF-8&quot;, LC_NUMERIC = &quot;zh_CN.UTF-8&quot;, LC_PAPER = &quot;zh_CN.UTF-8&quot;, LANG = &quot;en_US.UTF-8&quot; are supported and installed on your system. perl: warning: Falling back to a fallback locale (&quot;en_US.UTF-8&quot;). locale: Cannot set LC_CTYPE to default locale: No such file or directory locale: Cannot set LC_ALL to default locale: No such file or directory (Reading database ... 247161 files and directories currently installed.) Preparing to unpack .../git-man_1%3a2.13.0-0ppa1~ubuntu16.04.1_all.deb ... Unpacking git-man (1:2.13.0-0ppa1~ubuntu16.04.1) over (1:2.7.4-0ubuntu1.1) ... Selecting previously unselected package git. Preparing to unpack .../git_1%3a2.13.0-0ppa1~ubuntu16.04.1_amd64.deb ... Unpacking git (1:2.13.0-0ppa1~ubuntu16.04.1) ... Processing triggers for man-db (2.7.5-1) ... Setting up git-man (1:2.13.0-0ppa1~ubuntu16.04.1) ... Setting up git (1:2.13.0-0ppa1~ubuntu16.04.1) ... 查看git版本 johnathon@ubuntu16:~$ git --version git version 2.13.0 配置Git 设置用户名和邮箱johnathon@ubuntu16:~$ git config --global user.name &apos;*&apos; johnathon@ubuntu16:~$ git config --global user.email &apos;*@*.com&apos; home目录下会生成.gitconfig文件,其内容正是上面步骤配置的信息johnathon@ubuntu16:~$ ll .gitconfig -rw-rw-r-- 1 johnathon johnathon 58 Jul 22 17:17 .gitconfig 添加SSH keys到github 运行下面命令，会在.ssh目录下生成相应文件johnathon@ubuntu16:~$ ssh-keygen -t rsa -C &apos;youremail&apos; 查看生成的私钥／公钥johnathon@ubuntu16:~/.ssh$ ll total 20 drwx------ 2 johnathon johnathon 4096 Jul 22 17:25 ./ drwxr-xr-x 22 johnathon johnathon 4096 Jul 22 17:18 ../ -rw------- 1 johnathon johnathon 1675 Jul 22 17:25 id_rsa -rw-r--r-- 1 johnathon johnathon 404 Jul 22 17:25 id_rsa.pub -rw-r--r-- 1 johnathon johnathon 222 Jul 1 21:41 known_hosts 将id_rsa.pub中内容拷贝到github中,如下图]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use pyspark with python3]]></title>
    <url>%2FBigData%2FUse-pyspark-with-python3%2F</url>
    <content type="text"><![CDATA[edit profile :vim ~/.profile add the code into the file: export PYSPARK_PYTHON=python3 execute command : source ~/.profile ./bin/pyspark]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install Sqoop1.4.6]]></title>
    <url>%2FBigData%2FUbuntu16-04-Install-Sqoop1-4-6%2F</url>
    <content type="text"><![CDATA[安装环境OS: linux(ubuntu16.04)sqoop version: 1.4.6hadoop version: 2.8.0mysql version: 5.7.18 下载解压sqoop1.4.6 前往sqoop官网下载,默认下载目录为当前用户Downloads目录 johnathon@ubuntu16:~$ cd Downloads/ johnathon@ubuntu16:~/Downloads$ sudo tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /usr/local johnathon@ubuntu16:~/Downloads$ cd /usr/local johnathon@ubuntu16:/usr/local$ sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop johnathon@ubuntu16:/usr/local$ sudo chown -R hadoop:hadoop sqoop 修改配置文件 打开sqoop-env.sh hadoop@ubuntu16:~$ cd /usr/local/sqoop/conf/ hadoop@ubuntu16:/usr/local/sqoop/conf$ cp sqoop-env-template.sh sqoop-env.sh hadoop@ubuntu16:/usr/local/sqoop/conf$ vi sqoop-env.sh 添加以下信息 export HADOOP_COMMON_HOME=/usr/local/hadoop export HADOOP_MAPRED_HOME=/usr/local/hadoop export HIVE_HOME=/usr/local/hive 配置环境变量 打开~/.bashrc文件hadoop@ubuntu16:~$ vi ~/.bashrc 添加以下信息export SQOOP_HOME=/usr/local/sqoop export PATH=$PATH:$SBT_HOME/bin:$SQOOP_HOME/bin export CLASSPATH=$CLASSPATH:$SQOOP_HOME/lib 使修改生效 hadoop@ubuntu16:~$ source ~/.bashrc 添加mysql驱动到$SQOOP_HOME/lib下 johnathon@ubuntu16:~/Downloads/mysql-connector-java-5.1.42$ sudo cp mysql-connector-java-5.1.42-bin.jar /usr/local/sqoop/lib/ [sudo] password for johnathon: johnathon@ubuntu16:~/Downloads/mysql-connector-java-5.1.42$ cd /usr/local/sqoop/lib/ johnathon@ubuntu16:/usr/local/sqoop/lib$ ll mysql-connector-java-5.1.42-bin.jar -rw-r--r-- 1 root root 996444 Jul 16 17:25 mysql-connector-java-5.1.42-bin.jar 测试连接mysql 测试命令 sqoop list-databases --connect jdbc:mysql://127.0.0.1:3306/ --username root -P mysql数据库显示如下，则连接成功 hadoop@ubuntu16:~$ sqoop list-databases --connect jdbc:mysql://127.0.0.1:3306/ --username root -P Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 17/07/16 18:04:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6 Enter password: 17/07/16 18:04:12 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. Sun Jul 16 18:04:12 CST 2017 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. information_schema hive mysql performance_schema sys]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Problems in Using Hive]]></title>
    <url>%2FBigData%2Fprobles-in-using-hive%2F</url>
    <content type="text"><![CDATA[在使用beeline方式连接hive时，遇到的一个坑，困扰多时，在网上也搜了好久，还好没放弃，今天终于找到了答案，在此非常感谢 [Hive]那些年我们踩过的Hive坑(如有侵犯，还望告知),我的问题就是其中的第10个问题。 Question01 问题: Error: Could not open client transport with JDBC Uri: jdbc:hive2://192.168.140.128:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: hadoop is not allowed to impersonate hive (state=08S01,code=0) 解决方法:修改hadoop 配置文件 etc/hadoop/core-site.xml,加入如下配置项 &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The superuser can connect only from host1 and host2 to impersonate a user&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;Allow the superuser oozie to impersonate any members of the group group1 and group2&lt;/description&gt; &lt;/property&gt; 需要注意的是：hadoop.proxyuser.?.hosts 和 hadoop.proxyuser.?.groups中的 ? 和报错信息 User: ? is not allowed to impersonate hive (state=08S01,code=0)中的? 相对应，我这里是hadoop,所以我填的是hadoop.]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install and Configure Hive2.1.1]]></title>
    <url>%2FBigData%2FInstall-and-Configure-Hive2-1-1%2F</url>
    <content type="text"><![CDATA[准备工作 安装jdk 参考Ubuntu16.04 Install and Configure Oracle JDK 安装Hadoop 参考Ubuntu16.04 Install Hadoop 2.8.0 下载hive安装包前往hive官方下载地址 解压安装hive 解压hive安装文件到相应的目录hadoop@ubuntu16:~$ sudo tar -zxf apache-hive-2.1.1-bin.tar.gz -C /usr/local 给hive目录重命名hadoop@ubuntu16:~$ cd /usr/local hadoop@ubuntu16:/usr/local$ sudo mv apache-hive-2.1.1-bin/ hive 将hive目录用户改为hadoophadoop@ubuntu16:/usr/local$ sudo chown -R hadoop hive/ 查看hive目录hadoop@ubuntu16:/usr/local$ cd hive/ hadoop@ubuntu16:/usr/local/hadoop$ ls -l hadoop@ubuntu16:/usr/local/hive$ ll total 112 drwxr-xr-x 10 hadoop root 4096 Jul 8 22:11 ./ drwxr-xr-x 12 root root 4096 Jul 8 20:11 ../ -rw-r--r-- 1 hadoop staff 29003 Nov 29 2016 LICENSE -rw-r--r-- 1 hadoop staff 578 Nov 29 2016 NOTICE -rw-r--r-- 1 hadoop staff 4122 Nov 29 2016 README.txt -rw-r--r-- 1 hadoop staff 18501 Nov 30 2016 RELEASE_NOTES.txt drwxr-xr-x 3 hadoop root 4096 Jul 8 20:11 bin/ drwxr-xr-x 3 hadoop root 4096 Jul 8 22:15 conf/ drwxr-xr-x 4 hadoop root 4096 Jul 8 20:11 examples/ drwxr-xr-x 7 hadoop root 4096 Jul 8 20:11 hcatalog/ drwxr-xr-x 2 hadoop root 4096 Jul 8 20:11 jdbc/ drwxr-xr-x 4 hadoop root 12288 Jul 8 21:10 lib/ drwxr-xr-x 4 hadoop root 4096 Jul 8 20:11 scripts/ drwxrwxr-x 3 hadoop hadoop 4096 Jul 8 22:19 tmp/ 设置hive环境变量 打开~/.bashrc文件，并添加如下 # set hive env start export HIVE_HOME=/usr/local/hive export PATH=$PATH:$HIVE_HOME/bin export PATH=$PATH:$HIVE_HOME/hcatalog/bin:$HIVE_HOME/hcatalog/sbin export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib export HIVE_CONF_DIR=$HIVE_HOME/conf # set hive env end 使配置文件生效 hadoop@ubuntu16:~$ source ~/.bashrc 配置hive 修改默认配置文件名使配置文件生效 hadoop@ubuntu16:~$ cd /usr/local/hive/conf/ hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-env.sh.template hive-env.sh hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-default.xml.template hive-site.xml hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-log4j2.properties.template hive-log4j2.properties hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties 修改hive-env.sh,添加如下 export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export HADOOP_HOME=/usr/local/hadoop export HIVE_HOME=/usr/local/hive export HIVE_CONF_DIR=/usr/local/hive/conf 创建hdfs目录 hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/warehouse hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/tmp hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/log hadoop@ubuntu16:~$ hdfs dfs -chmod -R 777 /user/hive/warehouse hadoop@ubuntu16:~$ hdfs DFS -chmod -R 777 /user/hive/tmp hadoop@ubuntu16:~$ hdfs dfs -chmod -R 777 /user/hive/log hadoop@ubuntu16:~$ hdfs dfs -ls /user/hive Found 3 items drwxrwxrwx - hadoop supergroup 0 2017-07-08 20:50 /user/hive/log drwxrwxrwx - hadoop supergroup 0 2017-07-08 21:54 /user/hive/tmp drwxrwxrwx - hadoop supergroup 0 2017-07-08 20:50 /user/hive/warehouse 本地建立tmp目录 hadoop@ubuntu16:/usr/local/hive$ mkdir tmp 安装mysql数据库，并作相关配置获取最近的软件包的列表 hadoop@ubuntu16:~$ sudo apt-get update 安装mysql服务和客户端，中间会要求输入root密码 hadoop@ubuntu16:~$ sudo apt-get install mysql-server mysql-client root登陆mysql hadoop@ubuntu16:~$ mysql -u root -p 创建hive用户 mysql&gt; create user &apos;hive&apos; identified by &apos;hive&apos;; 查看数据库 mysql&gt; show databases; 创建数据库命名为hive mysql&gt; create database hive; 为hive用户授权 mysql&gt; grant all privileges on *.* to &apos;hive&apos;@&apos;localhost&apos; identified by &apos;hive&apos;; mysql&gt; flush privileges 退出root登陆 mysql&gt; exit; hive用户登录 hadoop@ubuntu16:~$ mysql -u hive -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 67 Server version: 5.7.18-0ubuntu0.16.04.1 (Ubuntu) Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; 修改hive-site.xml文件6.1 相关目录信息 &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/user/hive/tmp&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/user/hive/log&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; 6.2 ${system:java.io.tmpdir} 和 ${system:user.name} 分别替换成 /user/local/hive/tmp 和 ${user.name}6.3 mysql数据库连接信息,需要将mysql的jar包放入hive/lib目录下 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; 启动hive hadoop@ubuntu16:~$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. hive&gt;]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>mysql</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install Hadoop 2.8.0]]></title>
    <url>%2FBigData%2FUbuntu16-04-Install-Hadoop-2-8-0%2F</url>
    <content type="text"><![CDATA[创建hadoop用户 创建用户hadoop,并且用/bin/bash作为默认shell：johnathon@ubuntu16:~$ sudo useradd -m hadoop -s /bin/bash 为用户hadoop设置密码：johnathon@ubuntu16:~$ sudo passwd hadoop 将用户hadoop加入sudo组：johnathon@ubuntu16:~$ sudo adduser hadoop sudo 更新apt hadoop@ubuntu16:~$ sudo apt-get update 配置ssh免密登陆 安装ssh服务hadoop@ubuntu16:~$ sudo apt-get install ssh 创建.ssh目录hadoop@ubuntu16:~$ cd ~ hadoop@ubuntu16:~$ mkdir .ssh 生成ssh密钥hadoop@ubuntu16:~$ cd .ssh/ hadoop@ubuntu16:~$ ssh-keygen -t rsa hadoop@ubuntu16:~/.ssh$ cat id_rsa.pub &gt;&gt; authorized_keys ssh登陆localhosthadoop@ubuntu16:~/.ssh$ cd hadoop@ubuntu16:~$ ssh localhost 退出ssh登陆hadoop@ubuntu16:~$ exit logout Connection to localhost closed. 安装配置jdk 参考Ubuntu16.04 Install and Configure Oracle JDK 安装hadoop 解压hadoop安装文件到相应的目录hadoop@ubuntu16:~$ sudo tar -zxf hadoop-2.8.0.tar.gz -C /usr/local 给hadoop目录重命名hadoop@ubuntu16:~$ cd /usr/local hadoop@ubuntu16:/usr/local$ sudo mv hadoop-2.8.0/ hadoop 将hadoop目录用户改为hadoophadoop@ubuntu16:/usr/local$ sudo chown -R hadoop hadoop/ 查看hadoop目录hadoop@ubuntu16:/usr/local$ cd hadoop/ hadoop@ubuntu16:/usr/local/hadoop$ ls -l total 148 -rw-r--r-- 1 hadoop dialout 99253 Mar 17 13:31 LICENSE.txt -rw-r--r-- 1 hadoop dialout 15915 Mar 17 13:31 NOTICE.txt -rw-r--r-- 1 hadoop dialout 1366 Mar 17 13:31 README.txt drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 bin drwxr-xr-x 3 hadoop dialout 4096 Mar 17 13:31 etc drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 include drwxr-xr-x 3 hadoop dialout 4096 Mar 17 13:31 lib drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 libexec drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 sbin drwxr-xr-x 4 hadoop dialout 4096 Mar 17 13:31 share 查看hadoop版本信息 hadoop@ubuntu16:/usr/local/hadoop$ ./bin/hadoop version Hadoop 2.8.0 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009 Compiled by jdu on 2017-03-17T04:12Z Compiled with protoc 2.5.0 From source with checksum 60125541c2b3e266cbf3becc5bda666 This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.0.jar 设置hadoop环境变量 6.1. 打开hadoop用户下配置文件 hadoop@ubuntu16:~$ vi .bashrc 6.2. 编辑文件，添加如下 #set hadoop env begin export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$JAVA_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin #set hadoop env end 6.3. 使文件修改生效 hadoop@ubuntu16:~$ source .bashrc 6.4. 查看hadoop版本信息（注意：与上一步骤中查看版本信息的区别） hadoop@ubuntu16:~$ hadoop version Hadoop 2.8.0 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009 Compiled by jdu on 2017-03-17T04:12Z Compiled with protoc 2.5.0 From source with checksum 60125541c2b3e266cbf3becc5bda666 This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.0.jar 配置hadoop伪分布式配置hadoop伪分布式需要修改相关配置文件：hadoop-env.xml/core-site.xml/hdfs-site.xml hadoop配置文件位置：hadoop主目录下的/etc/hadoop,如下 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ ls capacity-scheduler.xml httpfs-env.sh mapred-env.sh configuration.xsl httpfs-log4j.properties mapred-queues.xml.template container-executor.cfg httpfs-signature.secret mapred-site.xml core-site.xml httpfs-site.xml mapred-site.xml.template hadoop-env.cmd kms-acls.xml slaves hadoop-env.sh kms-env.sh ssl-client.xml.example hadoop-metrics.properties kms-log4j.properties ssl-server.xml.example hadoop-metrics2.properties kms-site.xml yarn-env.cmd hadoop-policy.xml log4j.properties yarn-env.sh hdfs-site.xml mapred-env.cmd yarn-site.xml 修改相应的配置文件 打开hadoop-env.sh hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi hadoop-env.sh 找到以下位置，把java目录修改为自己的主目录 # The java implementation to use. export JAVA_HOME=${JAVA_HOME} 打开core-site.xml,并添加configuration内容 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 打开hdfs-site.xml,并添加configuration内容hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 格式化namenode hadoop@ubuntu16:~$ hdfs namenode -format 启动NameNode和DataNode进程 hadoop@ubuntu16:~$ start-dfs.sh Starting namenodes on [localhost] localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-ubuntu16.out localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-ubuntu16.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-ubuntu16.out hadoop@ubuntu16:~$ jps 41749 NameNode 42203 Jps 42092 SecondaryNameNode 41903 DataNode 访问web界面输入主机ip:50070,访问主界面，如下 运行hadoop实例 创建目录hadoop@ubuntu16:~$ hdfs dfs -mkdir /user/hadoop hadoop@ubuntu16:~$ hdfs dfs -mkdir input 将本地文件拷贝到hdfs下input目录（发现有警告信息，目前还没有找到解决方法，不管不影响运行结果） hadoop@ubuntu16:~$ hdfs dfs -put /usr/local/hadoop/etc/hadoop/*.xml input 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) hadoop@ubuntu16:~$ hdfs dfs -ls Found 1 items drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:00 input hadoop@ubuntu16:~$ hdfs dfs -ls input Found 8 items -rw-r--r-- 1 hadoop supergroup 4942 2017-07-08 16:00 input/capacity-scheduler.xml -rw-r--r-- 1 hadoop supergroup 1032 2017-07-08 16:00 input/core-site.xml -rw-r--r-- 1 hadoop supergroup 9683 2017-07-08 16:00 input/hadoop-policy.xml -rw-r--r-- 1 hadoop supergroup 1079 2017-07-08 16:00 input/hdfs-site.xml -rw-r--r-- 1 hadoop supergroup 620 2017-07-08 16:00 input/httpfs-site.xml -rw-r--r-- 1 hadoop supergroup 3518 2017-07-08 16:00 input/kms-acls.xml -rw-r--r-- 1 hadoop supergroup 5546 2017-07-08 16:00 input/kms-site.xml -rw-r--r-- 1 hadoop supergroup 794 2017-07-08 16:00 input/yarn-site.xml 运行自带实例 hadoop@ubuntu16:~$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.0.jar grep input output ‘dfs[a-z.]+’ 查看运行结果 hadoop@ubuntu16:~$ hdfs dfs -ls Found 2 items drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:00 input drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:10 output hadoop@ubuntu16:~$ hdfs dfs -ls output Found 2 items -rw-r--r-- 1 hadoop supergroup 0 2017-07-08 16:10 output/_SUCCESS -rw-r--r-- 1 hadoop supergroup 77 2017-07-08 16:10 output/part-r-00000 hadoop@ubuntu16:~$ hdfs dfs -cat output/* 1 dfsadmin 1 dfs.replication 1 dfs.namenode.name.dir 1 dfs.datanode.data.dir NOTE: 再次运行前，需要删除掉output目录，否则会报错 配置yarn 打开yarn-site.xml,并添加如下configuration &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动ResourceManager和NodeManager进程,jps可以看出比单独启动start-dfs.sh 多出来两个进程 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-ubuntu16.out localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-ubuntu16.out hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ jps 46788 SecondaryNameNode 51397 NodeManager 46598 DataNode 46443 NameNode 51724 Jps 51276 ResourceManager 启动历史进程hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ mr-jobhistory-daemon.sh start historyserver starting historyserver, logging to /usr/local/hadoop/logs/mapred-hadoop-historyserver-ubuntu16.out 访问web界面输入主机ip:8088,访问主界面，如下 关闭所有进程,和开启顺序相反 hadoop@ubuntu16:~$ mr-jobhistory-daemon.sh stop historyserver stopping historyserver hadoop@ubuntu16:~$ stop-yarn.sh stopping yarn daemons stopping resourcemanager localhost: stopping nodemanager localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9 no proxyserver to stop hadoop@ubuntu16:~$ stop-dfs.sh Stopping namenodes on [localhost] localhost: stopping namenode localhost: stopping datanode Stopping secondary namenodes [0.0.0.0] 0.0.0.0: stopping secondarynamenode]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install and Configure Oracle JDK]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2Fubuntu-install-jdk-without-source-everytime-open-a-new-shell%2F</url>
    <content type="text"><![CDATA[查看系统位数，终端输入： getconf LONG_BIT 下载对应版本的jdk，这里下载的是jdk-8u131-linux-x64.tar.gz 创建目录作为jdk安装目录，这里选择安装位置为：~/Java（可自行选择安装路径） sudo mkdir Java 解压文件到上一步创建的目录~/Java目录下,JDK默认下载路径为Downloads目录 cd ~/Downloads sudo tar -zxvf jdk-8u131-linux-x64.tar.gz -C ~/Java 配置系统环境变量(全局：/etc/profile|当前用户：~/.bashrc) sudo vi /etc/profile 在最后加入: ##config java environment start export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib:$CLASSPATH export PATH=${JAVA_HOME}/bin:${JAVA_HOME}/jre/bin:$PATH ##config java environment end 修改完成后，保存并关闭，输入一下命令使环境变量生效 source /etc/profile 查看安装版本： java -version 本以为到此就结束了，结果发现每次重新打开一个terminal，就找不到java环境，搜索发现做以下配置: 配置/etc/bash.bashrc sudo vi /etc/bash.bashrc 在最后加入： ##config java environment start export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib:$CLASSPATH export PATH=${JAVA_HOME}/bin:${JAVA_HOME}/jre/bin:$PATH ##config java environment end 改完成后，保存并关闭，输入一下命令使环境变量生效 source /etc/bash.bashrc 到此完成安装配置jdk。]]></content>
      <categories>
        <category>配置相关</category>
      </categories>
      <tags>
        <tag>jdk</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacPro Connect Ubuntu16.04 In VWware Fusion]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2FMac-Connect-Ubuntu-In-VWware-Fusion%2F</url>
    <content type="text"><![CDATA[Mac 和 Linux虚拟机互通开启ssh服务 查看是否安装ssh服务：ps -e | grep ssh 安装ssh服务：sudo apt-get install ssh mac终端ssh连接Linux虚拟机:ssh user@remote[ip] 为了方便使用别名sshubt登陆 编辑mac下.bash_profile文件(需要root权限):sudo vi ~/.bash_profile 添加下面语句 alias sshubt=&apos;ssh myusername@192.168.0.0&apos; 使用别名sshubt，输入连接到的Linux主机密码，登陆即可jockie:~$ sshubt johnathon@192.168.140.128&apos;s password: Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.8.0-36-generic x86_64) mac上传文件址Linux虚拟机scp ~/local/file user@remtoe:~/file ~/local/file: mac文件路径 user@remote:~/file: 服务器文件路径]]></content>
      <categories>
        <category>配置相关</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ubuntu</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[long time no see]]></title>
    <url>%2Funcategorized%2Flong-time-no-see%2F</url>
    <content type="text"><![CDATA[It’s been almost 3 years since I met you ,hexo! NOW,It’s Time to Come Back!!!]]></content>
  </entry>
</search>