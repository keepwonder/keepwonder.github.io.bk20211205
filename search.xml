<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python001-import this]]></title>
    <url>%2FPython%2Fpython001-import-this%2F</url>
    <content type="text"><![CDATA[import thisThe Zen of Python, by Tim Peters Beautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren’t special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one– and preferably only one –obvious way to do it.Although that way may not be obvious at first unless you’re Dutch.Now is better than never.Although never is often better than right now.If the implementation is hard to explain, it’s a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea – let’s do more of those!]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7 install mysql5.7]]></title>
    <url>%2FBigData%2Fcentos7-install-mysql5-7%2F</url>
    <content type="text"><![CDATA[卸载系统自带的Mariadb[root@localhost ~]# rpm -qa | grep mariadb mariadb-libs-5.5.52-1.el7.x86_64 [root@localhost ~]# rpm -e --nodeps mariadb-libs-5.5.52-1.el7.x86_64 [root@localhost ~]# rpm -qa | grep mariadb [root@localhost ~]# 删除etc目录下的my.cnf文件[root@localhost ~]# rm /etc/my.cnf rm: cannot remove ‘/etc/my.cnf’: No such file or directory [root@localhost ~]# 检查mysql是否存在[root@localhost ~]# rpm -qa | grep mysql [root@localhost ~]# 检查mysql组和用户是否存在，如无创建[root@localhost ~]# cat /etc/group | grep mysql [root@localhost ~]# cat /etc/passwd | grep mysql [root@localhost ~]# 创建mysql用户组[root@localhost ~]# groupadd mysql [root@localhost ~]# cat /etc/group | grep mysql mysql:x:1001: 创建一个用户名为mysql的用户并加入mysql用户组[root@localhost ~]# useradd -g mysql mysql [root@localhost ~]# cat /etc/passwd | grep mysql mysql:x:1001:1001::/home/mysql:/bin/bash [root@localhost ~]# 指定password 为 mysql[root@localhost ~]# passwd mysql Changing password for user mysql. New password: BAD PASSWORD: The password is shorter than 8 characters Retype new password: passwd: all authentication tokens updated successfully. [root@localhost ~]# 将mysql安装到/usr/local/bigdata下[root@localhost bigdata]# ll total 625636 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 -rw-r--r--. 1 root root 640650826 Aug 27 05:55 mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost bigdata]# tar -zxf mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost bigdata]# ll total 625636 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 root root 129 Aug 27 05:57 mysql-5.7.19-linux-glibc2.12-x86_64 -rw-r--r--. 1 root root 640650826 Aug 27 05:55 mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost bigdata]# mv mysql-5.7.19-linux-glibc2.12-x86_64 mysql57 [root@localhost bigdata]# ll total 625636 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 root root 129 Aug 27 05:57 mysql57 -rw-r--r--. 1 root root 640650826 Aug 27 05:55 mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz [root@localhost] ## 更改所属的组和用户 [root@localhost bigdata]# chown -R mysql mysql57/ [root@localhost bigdata]# ll total 0 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 mysql root 129 Aug 27 05:57 mysql57 [root@localhost bigdata]# chgrp -R mysql mysql57/ [root@localhost bigdata]# ll total 0 drwxr-xr-x. 10 hd hd 161 Aug 27 01:24 hadoop drwxr-xr-x. 8 hd hd 255 Mar 15 04:35 jdk1.8 drwxr-xr-x. 9 mysql mysql 129 Aug 27 05:57 mysql57 root@localhost mysql57]# mkdir data [root@localhost mysql57]# chown -R mysql:mysql data [root@localhost mysql57]# ll total 36 drwxr-xr-x. 2 mysql mysql 4096 Aug 27 05:57 bin -rw-r--r--. 1 mysql mysql 17987 Jun 22 10:13 COPYING drwxr-xr-x. 2 mysql mysql 6 Aug 27 06:02 data drwxr-xr-x. 2 mysql mysql 55 Aug 27 05:57 docs drwxr-xr-x. 3 mysql mysql 4096 Aug 27 05:57 include drwxr-xr-x. 5 mysql mysql 229 Aug 27 05:57 lib drwxr-xr-x. 4 mysql mysql 30 Aug 27 05:57 man -rw-r--r--. 1 mysql mysql 2478 Jun 22 10:13 README drwxr-xr-x. 28 mysql mysql 4096 Aug 27 05:57 share drwxr-xr-x. 2 mysql mysql 90 Aug 27 05:57 support-files [root@localhost mysql57]# 在etc下新建配置文件my.cnf，并在该文件内添加以下配置[mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 [mysqld] skip-name-resolve #设置3306端口 port = 3306 # 设置mysql的安装目录 basedir=/usr/local/bigdata/mysql57 # 设置mysql数据库的数据的存放目录 datadir=/usr/local/bigdata/mysql57/data # 允许最大连接数 max_connections=200 # 服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB lower_case_table_names=1 max_allowed_packet=16M 安装和初始化[root@localhost mysql57]# bin/mysql_install_db --user=mysql --basedir=/usr/local/bigdata/mysql57 --datadir=/usr/local/bigdata/mysql57/data/ 2017-08-27 06:08:18 [WARNING] mysql_install_db is deprecated. Please consider switching to mysqld --initialize 2017-08-27 06:08:20 [WARNING] The bootstrap log isn&apos;t empty: 2017-08-27 06:08:20 [WARNING] 2017-08-27T10:08:18.703344Z 0 [Warning] --bootstrap is deprecated. Please consider using --initialize instead 2017-08-27T10:08:18.705509Z 0 [Warning] Changed limits: max_open_files: 1024 (requested 5000) 2017-08-27T10:08:18.705523Z 0 [Warning] Changed limits: table_open_cache: 407 (requested 2000) [root@localhost mysql57]# [root@localhost mysql57]# cp ./support-files/mysql.server /etc/init.d/mysqld [root@localhost mysql57]# chown 777 /etc/my.cnf [root@localhost mysql57]# chmod +x /etc/init.d/mysqld [root@localhost mysql57]# /etc/init.d/mysqld restart ERROR! MySQL server PID file could not be found! Starting MySQL.Logging to &apos;/usr/local/bigdata/mysql57/data/localhost.localdomain.err&apos;. SUCCESS! 设置开机启动 [root@localhost mysql57]# chkconfig --level 35 mysqld on [root@localhost mysql57]# chkconfig --list mysqld Note: This output shows SysV services only and does not include native systemd services. SysV configuration data might be overridden by native systemd configuration. If you want to list systemd services use &apos;systemctl list-unit-files&apos;. To see services enabled on particular target use &apos;systemctl list-dependencies [target]&apos;. mysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off [root@localhost mysql57]# chmod +x /etc/rc.d/init.d/mysqld [root@localhost mysql57]# chkconfig --add mysqld [root@localhost mysql57]# chkconfig --list mysqld Note: This output shows SysV services only and does not include native systemd services. SysV configuration data might be overridden by native systemd configuration. If you want to list systemd services use &apos;systemctl list-unit-files&apos;. To see services enabled on particular target use &apos;systemctl list-dependencies [target]&apos;. mysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off [root@localhost mysql57]# service mysqld status SUCCESS! MySQL running (4822) [root@localhost mysql57]# 设置/etc/profile/export PATH=$PATH:/usr/local/bigdata/mysql557/bin 获得初始密码[root@localhost mysql57]# cat /root/.mysql_secret # Password set for user &apos;root@localhost&apos; at 2017-08-27 06:08:18 zm&amp;nSHM5Etpw 修改密码root@localhost mysql57]# mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 4 Server version: 5.7.19 Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; set PASSWORD = PASSWORD(&apos;root&apos;) -&gt; ; Query OK, 0 rows affected, 1 warning (0.01 sec) mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec) mysql&gt; 添加远程访问权限mysql&gt; use mysql Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; update user set host=&apos;%&apos; where user=&apos;root&apos;; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql&gt; select host,user from user; +-----------+---------------+ | host | user | +-----------+---------------+ | % | root | | localhost | mysql.session | | localhost | mysql.sys | +-----------+---------------+ 3 rows in set (0.00 sec) 重启生效[root@localhost mysql57]# systemctl restart mysql.service [root@localhost mysql57]# /etc/init.d/mysqld restart Shutting down MySQL.. SUCCESS! Starting MySQL. SUCCESS! 为了在任何目录下可以登录mysql[root@localhost mysql57]# ln -s /usr/local/bigdata/mysql57/bin/mysql /usr/bin/mysql]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>mysql5.7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install and Configure HBase]]></title>
    <url>%2FBigData%2FUbuntu16-04-Install-and-Configure-HBase%2F</url>
    <content type="text"><![CDATA[HBase安装 下载安装包从HBase官网下载最新稳定版，目前为hbase-1.2.6-bin.tar.gz 解压安装包将安装包解压至/usr/local目录下，并重命名为hbase,并给hbase目录用户和所属组都改为hadoop hadoop@ubuntu16:/usr/local$ sudo chown -R hadoop:hadoop hbase hadoop@ubuntu16:/usr/local$ ll | grep hbase drwxrwxr-x 9 hadoop hadoop 4096 8月 19 19:59 hbase/ hadoop@ubuntu16:/usr/local$ cd hbase/ hadoop@ubuntu16:/usr/local/hbase$ ll total 356 drwxrwxr-x 9 hadoop hadoop 4096 8月 19 19:59 ./ drwxr-xr-x 15 root root 4096 8月 19 19:42 ../ drwxr-xr-x 4 hadoop hadoop 4096 1月 29 2016 bin/ -rw-r--r-- 1 hadoop hadoop 129552 5月 29 14:29 CHANGES.txt drwxr-xr-x 2 hadoop hadoop 4096 8月 19 19:58 conf/ drwxr-xr-x 12 hadoop hadoop 4096 5月 29 15:20 docs/ drwxrwxr-x 7 hadoop hadoop 4096 8月 19 19:59 hbase-tmp/ drwxr-xr-x 7 hadoop hadoop 4096 5月 29 14:53 hbase-webapps/ -rw-rw-r-- 1 hadoop hadoop 261 5月 29 15:31 LEGAL drwxrwxr-x 3 hadoop hadoop 4096 8月 19 19:41 lib/ -rw-rw-r-- 1 hadoop hadoop 143082 5月 29 15:31 LICENSE.txt drwxrwxr-x 2 hadoop hadoop 4096 8月 19 19:59 logs/ -rw-rw-r-- 1 hadoop hadoop 42115 5月 29 15:31 NOTICE.txt -rw-r--r-- 1 hadoop hadoop 1477 12月 27 2015 README.txt 配置安装路径修改hadoop用户下.bashrc文件，添加如下: export HBASE_HOME=/usr/local/hbase export PATH=$PATH:$HBASE_HOME/bin:$PATH 执行source命令使修改生效: hadoop@ubuntu16:~$ source .bashrc 验证安装是否成功 hadoop@ubuntu16:~$ hbase version HBase 1.2.6 Source code repository file:///home/busbey/projects/hbase/hbase-assembly/target/hbase-1.2.6 revision=Unknown Compiled by busbey on Mon May 29 02:25:32 CDT 2017 From source with checksum 7e8ce83a648e252758e9dae1fbe779c9 看到以上消息表示hbase成功安装。 HBase单机模式 配置/conf/hbase-env.sh配置JAVA_HOME和HBASE_MANAGES_ZK两个变量 export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export HBASE_MANAGES_ZK=true 配置/conf/hbase-site.xml在启动Hbase前需要设置属性hbase.rootdir，用于指定Hbase数据的存储位置，此处设置为HBase安装目录下的hbase-tmp文件夹即（file:///usr/local/hbase/hbase-tmp），配置如下： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///usr/local/hbase/hbase-tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动HBase 启动之前先jps查看下，这里hadoop已经运行，hbase单机模式可以不开启hadoop hadoop@ubuntu16:/usr/local/hbase/conf$ jps 16913 RunJar 14580 NodeManager 18182 Jps 14935 JobHistoryServer 14105 DataNode 14459 ResourceManager 14302 SecondaryNameNode 13951 NameNode 使用start-hbase.sh脚本启动hbase hadoop@ubuntu16:/usr/local/hbase/conf$ start-hbase.sh starting master, logging to /usr/local/hbase/logs/hbase-hadoop-master-ubuntu16.out 再jps查看下,多出个HMaster进程 hadoop@ubuntu16:/usr/local/hbase/conf$ jps 16913 RunJar 18307 HMaster 14580 NodeManager 14935 JobHistoryServer 18615 Jps 14105 DataNode 14459 ResourceManager 14302 SecondaryNameNode 13951 NameNode hadoop@ubuntu16:/usr/local/hbase/conf$ jps | grep HM 18307 HMaster 进入hbase shell 进入shell模式之后，通过status命令查看运行状态，通过exit退出shell hadoop@ubuntu16:/usr/local/hbase/conf$ hbase shell SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands. Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017 hbase(main):001:0&gt; status 1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load hbase(main):002:0&gt; exit hadoop@ubuntu16:/usr/local/hbase/conf$ 停止HBase hadoop@ubuntu16:/usr/local/hbase/conf$ stop-hbase.sh stopping hbase................. hadoop@ubuntu16:/usr/local/hbase/conf$ HBase伪分布式 配置/conf/hbase-env.sh添加如下 export HBASE_CLASSPATH=/usr/local/hadoop/conf 配置/conf/hbase-site.xml修改hbase.rootdir，将其指向localhost(与hdfs的端口保持一致)，并指定HBase在HDFS上的存储路径。将属性hbase.cluter.distributed设置为true。假设当前Hadoop集群运行在伪分布式模式下，且NameNode运行在9000端口； &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动HBase 启动之前，保证hadoop已经启动，可用jps查看进程 hadoop@ubuntu16:/usr/local/hbase/conf$ start-hbase.sh localhost: starting zookeeper, logging to /usr/local/hbase/bin/../logs/hbase-hadoop-zookeeper-ubuntu16.out starting master, logging to /usr/local/hbase/logs/hbase-hadoop-master-ubuntu16.out starting regionserver, logging to /usr/local/hbase/logs/hbase-hadoop-1-regionserver-ubuntu16.out jps查看进程 hadoop@ubuntu16:/usr/local/hbase/conf$ jps 16913 RunJar 14580 NodeManager 23094 HQuorumPeer 14935 JobHistoryServer 23287 HRegionServer 23160 HMaster 23880 Jps 14105 DataNode 14459 ResourceManager 14302 SecondaryNameNode 13951 NameNode 再过滤下，可看出多出如下3个进程 hadoop@ubuntu16:/usr/local/hbase/conf$ jps | awk &apos;{print $2}&apos;| grep &apos;^H&apos; HQuorumPeer HRegionServer HMaster 进程shell模式进入shell模式之后，通过list命令查看当前数据库所有表信息，通过create命令创建一个employee表，其拥有employee_id,address,info三个列族，通过describe命令查看employee表结构，通过exit命令退出HBase shell模式。 hadoop@ubuntu16:/usr/local/hbase/conf$ hbase shell SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands. Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017 hbase(main):001:0&gt; create &apos;employee&apos;,&apos;employee_id&apos;,&apos;address&apos;,&apos;info&apos; 0 row(s) in 1.8040 seconds =&gt; Hbase::Table - employee hbase(main):006:0&gt; list TABLE employee 1 row(s) in 0.0080 seconds =&gt; [&quot;employee&quot;] hbase(main):007:0&gt; describe &apos;employee&apos; Table employee is ENABLED employee COLUMN FAMILIES DESCRIPTION {NAME =&gt; &apos;address&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;} {NAME =&gt; &apos;employee_id&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;fal se&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREV ER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;} {NAME =&gt; &apos;info&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KE EP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, CO MPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65 536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;} 3 row(s) in 0.0650 seconds hbase(main):008:0&gt; exit hadoop@ubuntu16:/usr/local/hbase/conf$ 查看HDFS的HBase数据库文件通过hdfs dfs –ls /hbase命令查看HBase分布式数据库在HDFS上是否成功创建，/hbase/data/default/employee文件夹即为上一步我们所建立的employee在HDFS上的存储位置。 hadoop@ubuntu16:/usr/local/hbase/conf$ hdfs dfs -ls /hbase Found 8 items drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/.tmp drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/MasterProcWALs drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:26 /hbase/WALs drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:38 /hbase/archive drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:10 /hbase/data -rw-r--r-- 1 hadoop supergroup 42 2017-08-19 21:10 /hbase/hbase.id -rw-r--r-- 1 hadoop supergroup 7 2017-08-19 21:10 /hbase/hbase.version drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/oldWALs hadoop@ubuntu16:/usr/local/hbase/conf$ hdfs dfs -ls /hbase/data/default Found 1 items drwxr-xr-x - hadoop supergroup 0 2017-08-19 21:36 /hbase/data/default/employee 通过界面查看相关信息主机ip:50070查看hdfs信息,如下图主机ip:16010查看hbase相关新，如下图 停止HBase hadoop@ubuntu16:/usr/local/hbase/conf$ stop-hbase.sh stopping hbase................... localhost: stopping zookeeper. hadoop@ubuntu16:/usr/local/hbase/conf$]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Command Line Series：A Command A Day 003]]></title>
    <url>%2FLinux%2Fa-command-a-day-003%2F</url>
    <content type="text"><![CDATA[day003: grepgrep（缩写来自Globally search a Regular Expression and Print）是一种强大的文本搜索工具，它能使用特定模式匹配（包括正则表达式）搜索文本，并默认输出匹配行 用–help查看grep命令相关信息 johnathon@ubuntu16:~/programs/shell$ grep --help Usage: grep [OPTION]... PATTERN [FILE]... Search for PATTERN in each FILE or standard input. PATTERN is, by default, a basic regular expression (BRE). Example: grep -i &apos;hello world&apos; menu.h main.c Regexp selection and interpretation: -E, --extended-regexp PATTERN is an extended regular expression (ERE) -F, --fixed-strings PATTERN is a set of newline-separated strings -G, --basic-regexp PATTERN is a basic regular expression (BRE) -P, --perl-regexp PATTERN is a Perl regular expression -e, --regexp=PATTERN use PATTERN for matching -f, --file=FILE obtain PATTERN from FILE -i, --ignore-case ignore case distinctions -w, --word-regexp force PATTERN to match only whole words -x, --line-regexp force PATTERN to match only whole lines -z, --null-data a data line ends in 0 byte, not newline Miscellaneous: -s, --no-messages suppress error messages -v, --invert-match select non-matching lines -V, --version display version information and exit --help display this help text and exit Output control: -m, --max-count=NUM stop after NUM matches -b, --byte-offset print the byte offset with output lines -n, --line-number print line number with output lines --line-buffered flush output on every line -H, --with-filename print the file name for each match -h, --no-filename suppress the file name prefix on output --label=LABEL use LABEL as the standard input file name prefix -o, --only-matching show only the part of a line matching PATTERN -q, --quiet, --silent suppress all normal output --binary-files=TYPE assume that binary files are TYPE; TYPE is &apos;binary&apos;, &apos;text&apos;, or &apos;without-match&apos; -a, --text equivalent to --binary-files=text -I equivalent to --binary-files=without-match -d, --directories=ACTION how to handle directories; ACTION is &apos;read&apos;, &apos;recurse&apos;, or &apos;skip&apos; -D, --devices=ACTION how to handle devices, FIFOs and sockets; ACTION is &apos;read&apos; or &apos;skip&apos; -r, --recursive like --directories=recurse -R, --dereference-recursive likewise, but follow all symlinks --include=FILE_PATTERN search only files that match FILE_PATTERN --exclude=FILE_PATTERN skip files and directories matching FILE_PATTERN --exclude-from=FILE skip files matching any file pattern from FILE --exclude-dir=PATTERN directories that match PATTERN will be skipped. -L, --files-without-match print only names of FILEs containing no match -l, --files-with-matches print only names of FILEs containing matches -c, --count print only a count of matching lines per FILE -T, --initial-tab make tabs line up (if needed) -Z, --null print 0 byte after FILE name Context control: -B, --before-context=NUM print NUM lines of leading context -A, --after-context=NUM print NUM lines of trailing context -C, --context=NUM print NUM lines of output context -NUM same as --context=NUM --color[=WHEN], --colour[=WHEN] use markers to highlight the matching strings; WHEN is &apos;always&apos;, &apos;never&apos;, or &apos;auto&apos; -U, --binary do not strip CR characters at EOL (MSDOS/Windows) -u, --unix-byte-offsets report offsets as if CRs were not there (MSDOS/Windows) &apos;egrep&apos; means &apos;grep -E&apos;. &apos;fgrep&apos; means &apos;grep -F&apos;. Direct invocation as either &apos;egrep&apos; or &apos;fgrep&apos; is deprecated. When FILE is -, read standard input. With no FILE, read . if a command-line -r is given, - otherwise. If fewer than two FILEs are given, assume -h. Exit status is 0 if any line is selected, 1 otherwise; if any error occurs and -q is not given, the exit status is 2. Report bugs to: bug-grep@gnu.org GNU grep home page: &lt;http://www.gnu.org/software/grep/&gt; General help using GNU software: &lt;http://www.gnu.org/gethelp/&gt;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[whois]]></title>
    <url>%2Funcategorized%2Fpython-whois%2F</url>
    <content type="text"><![CDATA[在爬取一个网站时，又是我们需要查看一个网站的所有者相关信息，这是酒可以用到whois这个模块step1: 使用pip install python-whois安装whois模块(/Users/jockie/install_programs/anaconda) jockie:~/programs/pycharm$ pip install python-whois Collecting python-whois Downloading python-whois-0.6.5.tar.gz Collecting future (from python-whois) Downloading future-0.16.0.tar.gz (824kB) 100% |################################| 829kB 137kB/s Building wheels for collected packages: python-whois, future Running setup.py bdist_wheel for python-whois ... done Stored in directory: /Users/jockie/Library/Caches/pip/wheels/37/68/27/819a3f07cbe75200d8cfa74d4517fd0f402b6dd7aaf91afe8b Running setup.py bdist_wheel for future ... done Stored in directory: /Users/jockie/Library/Caches/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a Successfully built python-whois future Installing collected packages: future, python-whois Successfully installed future-0.16.0 python-whois-0.6.5 step2: 使用whoisIn [1]: import whois In [2]: whois.whois(&apos;xuanxiewu.com&apos;) Out[2]: {&apos;address&apos;: &apos;NanJingShiYuHuaTaiQuXiShanQiaoMeiXinXiaoQu&apos;, &apos;city&apos;: &apos;Nanjing&apos;, &apos;country&apos;: &apos;CN&apos;, &apos;creation_date&apos;: datetime.datetime(2014, 9, 28, 4, 9, 31), &apos;dnssec&apos;: &apos;unsigned&apos;, &apos;domain_name&apos;: [&apos;XUANXIEWU.COM&apos;, &apos;xuanxiewu.com&apos;], &apos;emails&apos;: [&apos;tld@cndns.com&apos;, &apos;domain@cndns.com&apos;, &apos;1044699649@qq.com&apos;], &apos;expiration_date&apos;: [datetime.datetime(2018, 9, 28, 4, 9, 31), datetime.datetime(2018, 9, 28, 12, 7, 1)], &apos;name&apos;: &apos;qiang jiong&apos;, &apos;name_servers&apos;: [&apos;F1G1NS1.DNSPOD.NET&apos;, &apos;F1G1NS2.DNSPOD.NET&apos;, &apos;f1g1ns1.dnspod.net&apos;, &apos;f1g1ns2.dnspod.net&apos;], &apos;org&apos;: &apos;qiang jiong&apos;, &apos;referral_url&apos;: None, &apos;registrar&apos;: &apos;SHANGHAI MEICHENG TECHNOLOGY INFORMATION DEVELOPMENT CO., LTD.&apos;, &apos;state&apos;: &apos;Jiangsu&apos;, &apos;status&apos;: [&apos;clientTransferProhibited https://icann.org/epp#clientTransferProhibited&apos;, &apos;ok https://icann.org/epp#ok&apos;], &apos;updated_date&apos;: [datetime.datetime(2017, 6, 19, 15, 33, 50), datetime.datetime(2017, 6, 19, 23, 33, 51)], &apos;whois_server&apos;: &apos;grs-whois.cndns.com&apos;, &apos;zipcode&apos;: &apos;210041&apos;}]]></content>
      <tags>
        <tag>spider</tag>
        <tag>python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use builtwith in python3]]></title>
    <url>%2FDataAnalysis%2Fuse-builtwith-in-python3%2F</url>
    <content type="text"><![CDATA[python3中使用builtwith模块（使用工具pycharm,命令行也是pycharm自带terminal）step1: 使用pip install builtwith 来安装builtwith模块(/Users/jockie/install_programs/anaconda) jockie:~/programs/pycharm$ pip install builtwith Collecting builtwith Downloading builtwith-1.3.2.tar.gz Building wheels for collected packages: builtwith Running setup.py bdist_wheel for builtwith ... done Stored in directory: /Users/jockie/Library/Caches/pip/wheels/e4/cf/86/aa813feb4c79e680590a42766642b130358a01f1e26ecfe1d6 Successfully built builtwith Installing collected packages: builtwith Successfully installed builtwith-1.3.2 step2: 测试builtwith模块import builtwith info = builtwith.parse(&apos;http://www.xuanxiewu.com&apos;) print(info) 运行代码报如下错误 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py Traceback (most recent call last): File &quot;/Users/jockie/programs/pycharm/python_spider/chp01_01.py&quot;, line 8, in &lt;module&gt; import builtwith File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 42 except Exception , e: ^ SyntaxError: invalid syntax Process finished with exit code 1 可以看出报的是语法错误，那为什么会有语法错误呢？原因是builtwith是基于python2.x版本的，所以这里需要做一些相应的语法修改1.python2的‘Exception , e’写法不支持， 修改为Exception as e2.python2的print表达式，修改为print()函数3.builtwith使用的urllib2模块属于python2，python3中使用urllib,所以在init.py源码中使用urllib2的地方都需要改urllib的写法，首先需要将 import urllib2替换成 import urllib.request import urllib.error 再将urllib2相关方法替换 request = urllib.request.Request(url, None, {&apos;User-Agent&apos;: user_agent}) # request = urllib2.Request(url, None, {&apos;User-Agent&apos;: user_agent}) response = urllib.request.urlopen(request) # response = urllib2.urlopen(request) 再次运行代码,报如下错误： /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py Traceback (most recent call last): File &quot;/Users/jockie/programs/pycharm/python_spider/chp01_01.py&quot;, line 10, in &lt;module&gt; info = builtwith.parse(&apos;http://www.baidu.com&apos;) File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 69, in builtwith if contains(html, snippet): File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 111, in contains return re.compile(regex.split(&apos;\\;&apos;)[0], flags=re.IGNORECASE).search(v) TypeError: cannot use a string pattern on a bytes-like object Process finished with exit code 1 可以看出报的是类型错误，这是因为urllib返回的数据格式已经发生了改变，需要进行转码，将下面的代码 if html is None: html = response.read() 改为 if html is None: html = response.read() html = html.decode(&apos;utf-8&apos;) 再次运行代码，得到正确结果 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py {&apos;font-scripts&apos;: [&apos;Font Awesome&apos;, &apos;Google Font API&apos;], &apos;web-frameworks&apos;: [&apos;Twitter Bootstrap&apos;], &apos;javascript-frameworks&apos;: [&apos;jQuery&apos;]} Process finished with exit code 0 但是，再看上面的解码使用的是utf-8，写死了，如果网站用的不是utf-8呢，这里再试验下，以www.163.com为例，使用的是gbk,再次运行，又报如下错误 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py Error: &apos;utf-8&apos; codec can&apos;t decode byte 0xcd in position 565: invalid continuation byte Traceback (most recent call last): File &quot;/Users/jockie/programs/pycharm/python_spider/chp01_01.py&quot;, line 10, in &lt;module&gt; info = builtwith.parse(&apos;http://www.163.com&apos;) File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 69, in builtwith if contains(html, snippet): File &quot;/Users/jockie/install_programs/anaconda/lib/python3.6/site-packages/builtwith/__init__.py&quot;, line 111, in contains return re.compile(regex.split(&apos;\\;&apos;)[0], flags=re.IGNORECASE).search(v) TypeError: cannot use a string pattern on a bytes-like object Process finished with exit code 1 将编码改为gbk，得到正确结果 /Users/jockie/install_programs/anaconda/bin/python.app /Users/jockie/programs/pycharm/python_spider/chp01_01.py {&apos;web-servers&apos;: [&apos;Nginx&apos;]} Process finished with exit code 0 那么问题来了，不同的网站编码不一定相同，如果每次换一个网站，就要改一遍编码的话，那将增加许多额外的工作量，也是不现实的，那么有没有方法做到一劳永逸呢，这里就需要引入chardet模块,同样使用：pip install chardet,将builtwith源码，做如下修改 if html is None: html = response.read() # html = html.decode(&apos;utf-8&apos;) # add by Johnahton 20170805 encode_type = chardet.detect(html) if encode_type[&apos;encoding&apos;] == &apos;utf-8&apos;: html = html.decode(&apos;utf-8&apos;) else: html = html.decode(&apos;gbk&apos;) 加入chardet判断字符编码后，就可以一劳永逸了！]]></content>
      <categories>
        <category>DataAnalysis</category>
      </categories>
      <tags>
        <tag>spider</tag>
        <tag>python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install Git]]></title>
    <url>%2Ftools%2FUbuntu16-04-install-git%2F</url>
    <content type="text"><![CDATA[安装Gitubuntu系统安装软件一般直接apt-get install就OK了,所以可以直接用: johnathon@ubuntu16:~$ sudo apt-get install git 查看版本： johnathon@ubuntu16:~$ git --version git version 2.7.4 当前最新版本并不是2.7.4，而是2.13.3，所以直接这样安装的话并不能安装最新的git版本，正确的步骤按照以下进行 sudo add-apt-repository ppa:git-core/ppa johnathon@ubuntu16ohnathon@ubuntu16:~$ sudo add-apt-repository ppa:git-core/ppa The most current stable version of Git for Ubuntu. For release candidates, go to https://launchpad.net/~git-core/+archive/candidate . More info: https://launchpad.net/~git-core/+archive/ubuntu/ppa Press [ENTER] to continue or ctrl-c to cancel adding it gpg: keyring `/tmp/tmpbb9o0lni/secring.gpg&apos; created gpg: keyring `/tmp/tmpbb9o0lni/pubring.gpg&apos; created gpg: requesting key E1DF1F24 from hkp server keyserver.ubuntu.com gpg: /tmp/tmpbb9o0lni/trustdb.gpg: trustdb created gpg: key E1DF1F24: public key &quot;Launchpad PPA for Ubuntu Git Maintainers&quot; imported gpg: Total number processed: 1 gpg: imported: 1 (RSA: 1) OK 更新软件源，sudo apt-get update johnathon@ubuntu16:~$ sudo apt-get update Hit:1 http://cn.archive.ubuntu.com/ubuntu xenial InRelease Hit:2 http://cn.archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:3 http://cn.archive.ubuntu.com/ubuntu xenial-backports InRelease Get:4 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial InRelease [17.5 kB] Hit:5 http://security.ubuntu.com/ubuntu xenial-security InRelease Get:6 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main amd64 Packages [3256 B] Get:7 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main i386 Packages [3248 B] Get:8 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main Translation-en [2496 B] Fetched 26.5 kB in 1s (13.7 kB/s) Reading package lists... Done 安装git，sudo apt-get install git johnathon@ubuntu16:~$ sudo apt-get install git Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: linux-headers-4.8.0-36 linux-headers-4.8.0-36-generic linux-image-4.8.0-36-generic linux-image-extra-4.8.0-36-generic Use &apos;sudo apt autoremove&apos; to remove them. The following additional packages will be installed: git-man Suggested packages: git-daemon-run | git-daemon-sysvinit git-doc git-el git-email git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn The following NEW packages will be installed: git The following packages will be upgraded: git-man 1 upgraded, 1 newly installed, 0 to remove and 129 not upgraded. Need to get 6174 kB of archives. After this operation, 30.4 MB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main amd64 git-man all 1:2.13.0-0ppa1~ubuntu16.04.1 [1448 kB] Get:2 http://ppa.launchpad.net/git-core/ppa/ubuntu xenial/main amd64 git amd64 1:2.13.0-0ppa1~ubuntu16.04.1 [4726 kB] Fetched 6174 kB in 19s (322 kB/s) perl: warning: Setting locale failed. perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LC_TIME = &quot;zh_CN.UTF-8&quot;, LC_MONETARY = &quot;zh_CN.UTF-8&quot;, LC_CTYPE = &quot;UTF-8&quot;, LC_ADDRESS = &quot;zh_CN.UTF-8&quot;, LC_TELEPHONE = &quot;zh_CN.UTF-8&quot;, LC_NAME = &quot;zh_CN.UTF-8&quot;, LC_MEASUREMENT = &quot;zh_CN.UTF-8&quot;, LC_IDENTIFICATION = &quot;zh_CN.UTF-8&quot;, LC_NUMERIC = &quot;zh_CN.UTF-8&quot;, LC_PAPER = &quot;zh_CN.UTF-8&quot;, LANG = &quot;en_US.UTF-8&quot; are supported and installed on your system. perl: warning: Falling back to a fallback locale (&quot;en_US.UTF-8&quot;). locale: Cannot set LC_CTYPE to default locale: No such file or directory locale: Cannot set LC_ALL to default locale: No such file or directory (Reading database ... 247161 files and directories currently installed.) Preparing to unpack .../git-man_1%3a2.13.0-0ppa1~ubuntu16.04.1_all.deb ... Unpacking git-man (1:2.13.0-0ppa1~ubuntu16.04.1) over (1:2.7.4-0ubuntu1.1) ... Selecting previously unselected package git. Preparing to unpack .../git_1%3a2.13.0-0ppa1~ubuntu16.04.1_amd64.deb ... Unpacking git (1:2.13.0-0ppa1~ubuntu16.04.1) ... Processing triggers for man-db (2.7.5-1) ... Setting up git-man (1:2.13.0-0ppa1~ubuntu16.04.1) ... Setting up git (1:2.13.0-0ppa1~ubuntu16.04.1) ... 查看git版本 johnathon@ubuntu16:~$ git --version git version 2.13.0 配置Git 设置用户名和邮箱johnathon@ubuntu16:~$ git config --global user.name &apos;*&apos; johnathon@ubuntu16:~$ git config --global user.email &apos;*@*.com&apos; home目录下会生成.gitconfig文件,其内容正是上面步骤配置的信息johnathon@ubuntu16:~$ ll .gitconfig -rw-rw-r-- 1 johnathon johnathon 58 Jul 22 17:17 .gitconfig 添加SSH keys到github 运行下面命令，会在.ssh目录下生成相应文件johnathon@ubuntu16:~$ ssh-keygen -t rsa -C &apos;youremail&apos; 查看生成的私钥／公钥johnathon@ubuntu16:~/.ssh$ ll total 20 drwx------ 2 johnathon johnathon 4096 Jul 22 17:25 ./ drwxr-xr-x 22 johnathon johnathon 4096 Jul 22 17:18 ../ -rw------- 1 johnathon johnathon 1675 Jul 22 17:25 id_rsa -rw-r--r-- 1 johnathon johnathon 404 Jul 22 17:25 id_rsa.pub -rw-r--r-- 1 johnathon johnathon 222 Jul 1 21:41 known_hosts 将id_rsa.pub中内容拷贝到github中,如下图]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Command Line Series：A Command A Day 002]]></title>
    <url>%2FLinux%2Fa-command-a-day-002%2F</url>
    <content type="text"><![CDATA[Day002: man上一个命令介绍了help帮助命令，接下来这个命令和help有着类似的作用，那就是man命令，首先使用[man –help]命令查看手册 hadoop@ubuntu16:~$ man --help Usage: man [OPTION...] [SECTION] PAGE... -C, --config-file=FILE use this user configuration file -d, --debug emit debugging messages -D, --default reset all options to their default values --warnings[=WARNINGS] enable warnings from groff Main modes of operation: -f, --whatis equivalent to whatis -k, --apropos equivalent to apropos -K, --global-apropos search for text in all pages -l, --local-file interpret PAGE argument(s) as local filename(s) -w, --where, --path, --location print physical location of man page(s) -W, --where-cat, --location-cat print physical location of cat file(s) -c, --catman used by catman to reformat out of date cat pages -R, --recode=ENCODING output source page encoded in ENCODING Finding manual pages: -L, --locale=LOCALE define the locale for this particular man search -m, --systems=SYSTEM use manual pages from other systems -M, --manpath=PATH set search path for manual pages to PATH -S, -s, --sections=LIST use colon separated section list -e, --extension=EXTENSION limit search to extension type EXTENSION -i, --ignore-case look for pages case-insensitively (default) -I, --match-case look for pages case-sensitively --regex show all pages matching regex --wildcard show all pages matching wildcard --names-only make --regex and --wildcard match page names only, not descriptions -a, --all find all matching manual pages -u, --update force a cache consistency check --no-subpages don&apos;t try subpages, e.g. &apos;man foo bar&apos; =&gt; &apos;man foo-bar&apos; Controlling formatted output: -P, --pager=PAGER use program PAGER to display output -r, --prompt=STRING provide the `less&apos; pager with a prompt -7, --ascii display ASCII translation of certain latin1 chars -E, --encoding=ENCODING use selected output encoding --no-hyphenation, --nh turn off hyphenation --no-justification, --nj turn off justification -p, --preprocessor=STRING STRING indicates which preprocessors to run: e - [n]eqn, p - pic, t - tbl, g - grap, r - refer, v - vgrind -t, --troff use groff to format pages -T, --troff-device[=DEVICE] use groff with selected device -H, --html[=BROWSER] use www-browser or BROWSER to display HTML output -X, --gxditview[=RESOLUTION] use groff and display through gxditview (X11): -X = -TX75, -X100 = -TX100, -X100-12 = -TX100-12 -Z, --ditroff use groff and force it to produce ditroff -?, --help give this help list --usage give a short usage message -V, --version print program version Mandatory or optional arguments to long options are also mandatory or optional for any corresponding short options. Report bugs to cjwatson@debian.org. 接下来详细介绍每一个选项参数：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use pyspark with python3]]></title>
    <url>%2FBigData%2FUse-pyspark-with-python3%2F</url>
    <content type="text"><![CDATA[edit profile :vim ~/.profile add the code into the file: export PYSPARK_PYTHON=python3 execute command : source ~/.profile ./bin/pyspark]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install Sqoop1.4.6]]></title>
    <url>%2FBigData%2FUbuntu16-04-Install-Sqoop1-4-6%2F</url>
    <content type="text"><![CDATA[安装环境OS: linux(ubuntu16.04)sqoop version: 1.4.6hadoop version: 2.8.0mysql version: 5.7.18 下载解压sqoop1.4.6 前往sqoop官网下载,默认下载目录为当前用户Downloads目录 johnathon@ubuntu16:~$ cd Downloads/ johnathon@ubuntu16:~/Downloads$ sudo tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /usr/local johnathon@ubuntu16:~/Downloads$ cd /usr/local johnathon@ubuntu16:/usr/local$ sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop johnathon@ubuntu16:/usr/local$ sudo chown -R hadoop:hadoop sqoop 修改配置文件 打开sqoop-env.sh hadoop@ubuntu16:~$ cd /usr/local/sqoop/conf/ hadoop@ubuntu16:/usr/local/sqoop/conf$ cp sqoop-env-template.sh sqoop-env.sh hadoop@ubuntu16:/usr/local/sqoop/conf$ vi sqoop-env.sh 添加以下信息 export HADOOP_COMMON_HOME=/usr/local/hadoop export HADOOP_MAPRED_HOME=/usr/local/hadoop export HIVE_HOME=/usr/local/hive 配置环境变量 打开~/.bashrc文件hadoop@ubuntu16:~$ vi ~/.bashrc 添加以下信息export SQOOP_HOME=/usr/local/sqoop export PATH=$PATH:$SBT_HOME/bin:$SQOOP_HOME/bin export CLASSPATH=$CLASSPATH:$SQOOP_HOME/lib 使修改生效 hadoop@ubuntu16:~$ source ~/.bashrc 添加mysql驱动到$SQOOP_HOME/lib下 johnathon@ubuntu16:~/Downloads/mysql-connector-java-5.1.42$ sudo cp mysql-connector-java-5.1.42-bin.jar /usr/local/sqoop/lib/ [sudo] password for johnathon: johnathon@ubuntu16:~/Downloads/mysql-connector-java-5.1.42$ cd /usr/local/sqoop/lib/ johnathon@ubuntu16:/usr/local/sqoop/lib$ ll mysql-connector-java-5.1.42-bin.jar -rw-r--r-- 1 root root 996444 Jul 16 17:25 mysql-connector-java-5.1.42-bin.jar 测试连接mysql 测试命令 sqoop list-databases --connect jdbc:mysql://127.0.0.1:3306/ --username root -P mysql数据库显示如下，则连接成功 hadoop@ubuntu16:~$ sqoop list-databases --connect jdbc:mysql://127.0.0.1:3306/ --username root -P Warning: /usr/local/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 17/07/16 18:04:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6 Enter password: 17/07/16 18:04:12 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. Sun Jul 16 18:04:12 CST 2017 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. information_schema hive mysql performance_schema sys]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Problems in Using Hive]]></title>
    <url>%2FBigData%2Fprobles-in-using-hive%2F</url>
    <content type="text"><![CDATA[在使用beeline方式连接hive时，遇到的一个坑，困扰多时，在网上也搜了好久，还好没放弃，今天终于找到了答案，在此非常感谢 [Hive]那些年我们踩过的Hive坑(如有侵犯，还望告知),我的问题就是其中的第10个问题。 Question01 问题: Error: Could not open client transport with JDBC Uri: jdbc:hive2://192.168.140.128:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: hadoop is not allowed to impersonate hive (state=08S01,code=0) 解决方法:修改hadoop 配置文件 etc/hadoop/core-site.xml,加入如下配置项 &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The superuser can connect only from host1 and host2 to impersonate a user&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;Allow the superuser oozie to impersonate any members of the group group1 and group2&lt;/description&gt; &lt;/property&gt; 需要注意的是：hadoop.proxyuser.?.hosts 和 hadoop.proxyuser.?.groups中的 ? 和报错信息 User: ? is not allowed to impersonate hive (state=08S01,code=0)中的? 相对应，我这里是hadoop,所以我填的是hadoop.]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install and Configure Hive2.1.1]]></title>
    <url>%2FBigData%2FInstall-and-Configure-Hive2-1-1%2F</url>
    <content type="text"><![CDATA[准备工作 安装jdk 参考Ubuntu16.04 Install and Configure Oracle JDK 安装Hadoop 参考Ubuntu16.04 Install Hadoop 2.8.0 下载hive安装包前往hive官方下载地址 解压安装hive 解压hive安装文件到相应的目录hadoop@ubuntu16:~$ sudo tar -zxf apache-hive-2.1.1-bin.tar.gz -C /usr/local 给hive目录重命名hadoop@ubuntu16:~$ cd /usr/local hadoop@ubuntu16:/usr/local$ sudo mv apache-hive-2.1.1-bin/ hive 将hive目录用户改为hadoophadoop@ubuntu16:/usr/local$ sudo chown -R hadoop hive/ 查看hive目录hadoop@ubuntu16:/usr/local$ cd hive/ hadoop@ubuntu16:/usr/local/hadoop$ ls -l hadoop@ubuntu16:/usr/local/hive$ ll total 112 drwxr-xr-x 10 hadoop root 4096 Jul 8 22:11 ./ drwxr-xr-x 12 root root 4096 Jul 8 20:11 ../ -rw-r--r-- 1 hadoop staff 29003 Nov 29 2016 LICENSE -rw-r--r-- 1 hadoop staff 578 Nov 29 2016 NOTICE -rw-r--r-- 1 hadoop staff 4122 Nov 29 2016 README.txt -rw-r--r-- 1 hadoop staff 18501 Nov 30 2016 RELEASE_NOTES.txt drwxr-xr-x 3 hadoop root 4096 Jul 8 20:11 bin/ drwxr-xr-x 3 hadoop root 4096 Jul 8 22:15 conf/ drwxr-xr-x 4 hadoop root 4096 Jul 8 20:11 examples/ drwxr-xr-x 7 hadoop root 4096 Jul 8 20:11 hcatalog/ drwxr-xr-x 2 hadoop root 4096 Jul 8 20:11 jdbc/ drwxr-xr-x 4 hadoop root 12288 Jul 8 21:10 lib/ drwxr-xr-x 4 hadoop root 4096 Jul 8 20:11 scripts/ drwxrwxr-x 3 hadoop hadoop 4096 Jul 8 22:19 tmp/ 设置hive环境变量 打开~/.bashrc文件，并添加如下 # set hive env start export HIVE_HOME=/usr/local/hive export PATH=$PATH:$HIVE_HOME/bin export PATH=$PATH:$HIVE_HOME/hcatalog/bin:$HIVE_HOME/hcatalog/sbin export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib export HIVE_CONF_DIR=$HIVE_HOME/conf # set hive env end 使配置文件生效 hadoop@ubuntu16:~$ source ~/.bashrc 配置hive 修改默认配置文件名使配置文件生效 hadoop@ubuntu16:~$ cd /usr/local/hive/conf/ hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-env.sh.template hive-env.sh hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-default.xml.template hive-site.xml hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-log4j2.properties.template hive-log4j2.properties hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties 修改hive-env.sh,添加如下 export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export HADOOP_HOME=/usr/local/hadoop export HIVE_HOME=/usr/local/hive export HIVE_CONF_DIR=/usr/local/hive/conf 创建hdfs目录 hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/warehouse hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/tmp hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/log hadoop@ubuntu16:~$ hdfs dfs -chmod -R 777 /user/hive/warehouse hadoop@ubuntu16:~$ hdfs DFS -chmod -R 777 /user/hive/tmp hadoop@ubuntu16:~$ hdfs dfs -chmod -R 777 /user/hive/log hadoop@ubuntu16:~$ hdfs dfs -ls /user/hive Found 3 items drwxrwxrwx - hadoop supergroup 0 2017-07-08 20:50 /user/hive/log drwxrwxrwx - hadoop supergroup 0 2017-07-08 21:54 /user/hive/tmp drwxrwxrwx - hadoop supergroup 0 2017-07-08 20:50 /user/hive/warehouse 本地建立tmp目录 hadoop@ubuntu16:/usr/local/hive$ mkdir tmp 安装mysql数据库，并作相关配置获取最近的软件包的列表 hadoop@ubuntu16:~$ sudo apt-get update 安装mysql服务和客户端，中间会要求输入root密码 hadoop@ubuntu16:~$ sudo apt-get install mysql-server mysql-client root登陆mysql hadoop@ubuntu16:~$ mysql -u root -p 创建hive用户 mysql&gt; create user &apos;hive&apos; identified by &apos;hive&apos;; 查看数据库 mysql&gt; show databases; 创建数据库命名为hive mysql&gt; create database hive; 为hive用户授权 mysql&gt; grant all privileges on *.* to &apos;hive&apos;@&apos;localhost&apos; identified by &apos;hive&apos;; mysql&gt; flush privileges 退出root登陆 mysql&gt; exit; hive用户登录 hadoop@ubuntu16:~$ mysql -u hive -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 67 Server version: 5.7.18-0ubuntu0.16.04.1 (Ubuntu) Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; 修改hive-site.xml文件6.1 相关目录信息 &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/user/hive/tmp&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/user/hive/log&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; 6.2 ${system:java.io.tmpdir} 和 ${system:user.name} 分别替换成 /user/local/hive/tmp 和 ${user.name}6.3 mysql数据库连接信息,需要将mysql的jar包放入hive/lib目录下 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; 启动hive hadoop@ubuntu16:~$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. hive&gt;]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>mysql</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Command Line Series：A Command A Day 001]]></title>
    <url>%2FLinux%2Fa-command-a-day-001%2F</url>
    <content type="text"><![CDATA[Day001: help在我们遇到一个命令，不知道其具体用法的时候，help命令可能是我们使用的最多也最方便的命令了。 johnathon@ubuntu16:~$ help help help: help [-dms] [pattern ...] Display information about builtin commands. Displays brief summaries of builtin commands. If PATTERN is specified, gives detailed help on all commands matching PATTERN, otherwise the list of help topics is printed. Options: -d output short description for each topic -m display usage in pseudo-manpage format -s output only a short usage synopsis for each topic matching PATTERN Arguments: PATTERN Pattern specifiying a help topic Exit Status: Returns success unless PATTERN is not found or an invalid option is given. 简要的解释一下打印出来的信息 help: help [-dms] [pattern …] help命令后面可以跟[选项][参数]，可以看到选项和参数都是中括号扩起来的，说明都是可选的，也就意味着，可以直接输入help Display information about builtin commands. 直接翻译过来就是：显示内置命令信息 Displays brief summaries of builtin commands. If PATTERN is specified, gives detailed help on all commands matching PATTERN, otherwise the list of help topics is printed. 直接翻译过来就是：显示内置命令的简要概括信息。指定了PATTERN,就会给出所有符这个PATTERN命令的帮助信息，否则打印出help主题的列表信息 举例：pattern为p开头的内置命令,就显示出了popd/printf/pushd/pwd四个命令 johnathon@ubuntu16:~$ help p* Shell commands matching keyword `p*&apos; popd: popd [-n] [+N | -N] Remove directories from stack. Removes entries from the directory stack. With no arguments, removes the top directory from the stack, and changes to the new top directory. Options: -n Suppresses the normal change of directory when removing directories from the stack, so only the stack is manipulated. Arguments: +N Removes the Nth entry counting from the left of the list shown by `dirs&apos;, starting with zero. For example: `popd +0&apos; removes the first directory, `popd +1&apos; the second. -N Removes the Nth entry counting from the right of the list shown by `dirs&apos;, starting with zero. For example: `popd -0&apos; removes the last directory, `popd -1&apos; the next to last. The `dirs&apos; builtin displays the directory stack. Exit Status: Returns success unless an invalid argument is supplied or the directory change fails. printf: printf [-v var] format [arguments] Formats and prints ARGUMENTS under control of the FORMAT. Options: -v var assign the output to shell variable VAR rather than display it on the standard output FORMAT is a character string which contains three types of objects: plain characters, which are simply copied to standard output; character escape sequences, which are converted and copied to the standard output; and format specifications, each of which causes printing of the next successive argument. In addition to the standard format specifications described in printf(1), printf interprets: %b expand backslash escape sequences in the corresponding argument %q quote the argument in a way that can be reused as shell input %(fmt)T output the date-time string resulting from using FMT as a format string for strftime(3) The format is re-used as necessary to consume all of the arguments. If there are fewer arguments than the format requires, extra format specifications behave as if a zero value or null string, as appropriate, had been supplied. Exit Status: Returns success unless an invalid option is given or a write or assignment error occurs. pushd: pushd [-n] [+N | -N | dir] Add directories to stack. Adds a directory to the top of the directory stack, or rotates the stack, making the new top of the stack the current working directory. With no arguments, exchanges the top two directories. Options: -n Suppresses the normal change of directory when adding directories to the stack, so only the stack is manipulated. Arguments: +N Rotates the stack so that the Nth directory (counting from the left of the list shown by `dirs&apos;, starting with zero) is at the top. -N Rotates the stack so that the Nth directory (counting from the right of the list shown by `dirs&apos;, starting with zero) is at the top. dir Adds DIR to the directory stack at the top, making it the new current working directory. The `dirs&apos; builtin displays the directory stack. Exit Status: Returns success unless an invalid argument is supplied or the directory change fails. pwd: pwd [-LP] Print the name of the current working directory. Options: -L print the value of $PWD if it names the current working directory -P print the physical directory, without any symbolic links By default, `pwd&apos; behaves as if `-L&apos; were specified. Exit Status: Returns 0 unless an invalid option is given or the current directory cannot be read. 没有符合的pattern： johnathon@ubuntu16:~$ help haha* Shell commands matching keyword `haha*&apos; -bash: help: no help topics match `haha*&apos;. Try `help help&apos; or `man -k haha*&apos; or `info haha*&apos;. Options[选项]:-d output short description for each topic-m display usage in pseudo-manpage format-s output only a short usage synopsis for each topic matching PATTERN -d: 输出简短介绍信息 johnathon@ubuntu16:~$ help -d help help - Display information about builtin commands. -m: 以伪man页面格式显示使用信息 johnathon@ubuntu16:~$ help -m help NAME help - Display information about builtin commands. SYNOPSIS help [-dms] [pattern ...] DESCRIPTION Display information about builtin commands. Displays brief summaries of builtin commands. If PATTERN is specified, gives detailed help on all commands matching PATTERN, otherwise the list of help topics is printed. Options: -d output short description for each topic -m display usage in pseudo-manpage format -s output only a short usage synopsis for each topic matching PATTERN Arguments: PATTERN Pattern specifiying a help topic Exit Status: Returns success unless PATTERN is not found or an invalid option is given. SEE ALSO bash(1) IMPLEMENTATION GNU bash, version 4.3.48(1)-release (x86_64-pc-linux-gnu) Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; -s: 打印出help命令的梗概信息 johnathon@ubuntu16:~$ help -s help help: help [-dms] [pattern ...] Arguments[参数]: 指定需要帮助的主题的pattern Exit Status:Returns success unless PATTERN is not found or an invalid option is given.返回成功状态，除非pattern没有找到，或者输入了无效的选项]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install Hadoop 2.8.0]]></title>
    <url>%2FBigData%2FUbuntu16-04-Install-Hadoop-2-8-0%2F</url>
    <content type="text"><![CDATA[创建hadoop用户 创建用户hadoop,并且用/bin/bash作为默认shell：johnathon@ubuntu16:~$ sudo useradd -m hadoop -s /bin/bash 为用户hadoop设置密码：johnathon@ubuntu16:~$ sudo passwd hadoop 将用户hadoop加入sudo组：johnathon@ubuntu16:~$ sudo adduser hadoop sudo 更新apt hadoop@ubuntu16:~$ sudo apt-get update 配置ssh免密登陆 安装ssh服务hadoop@ubuntu16:~$ sudo apt-get install ssh 创建.ssh目录hadoop@ubuntu16:~$ cd ~ hadoop@ubuntu16:~$ mkdir .ssh 生成ssh密钥hadoop@ubuntu16:~$ cd .ssh/ hadoop@ubuntu16:~$ ssh-keygen -t rsa hadoop@ubuntu16:~/.ssh$ cat id_rsa.pub &gt;&gt; authorized_keys ssh登陆localhosthadoop@ubuntu16:~/.ssh$ cd hadoop@ubuntu16:~$ ssh localhost 退出ssh登陆hadoop@ubuntu16:~$ exit logout Connection to localhost closed. 安装配置jdk 参考Ubuntu16.04 Install and Configure Oracle JDK 安装hadoop 解压hadoop安装文件到相应的目录hadoop@ubuntu16:~$ sudo tar -zxf hadoop-2.8.0.tar.gz -C /usr/local 给hadoop目录重命名hadoop@ubuntu16:~$ cd /usr/local hadoop@ubuntu16:/usr/local$ sudo mv hadoop-2.8.0/ hadoop 将hadoop目录用户改为hadoophadoop@ubuntu16:/usr/local$ sudo chown -R hadoop hadoop/ 查看hadoop目录hadoop@ubuntu16:/usr/local$ cd hadoop/ hadoop@ubuntu16:/usr/local/hadoop$ ls -l total 148 -rw-r--r-- 1 hadoop dialout 99253 Mar 17 13:31 LICENSE.txt -rw-r--r-- 1 hadoop dialout 15915 Mar 17 13:31 NOTICE.txt -rw-r--r-- 1 hadoop dialout 1366 Mar 17 13:31 README.txt drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 bin drwxr-xr-x 3 hadoop dialout 4096 Mar 17 13:31 etc drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 include drwxr-xr-x 3 hadoop dialout 4096 Mar 17 13:31 lib drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 libexec drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 sbin drwxr-xr-x 4 hadoop dialout 4096 Mar 17 13:31 share 查看hadoop版本信息 hadoop@ubuntu16:/usr/local/hadoop$ ./bin/hadoop version Hadoop 2.8.0 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009 Compiled by jdu on 2017-03-17T04:12Z Compiled with protoc 2.5.0 From source with checksum 60125541c2b3e266cbf3becc5bda666 This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.0.jar 设置hadoop环境变量 6.1. 打开hadoop用户下配置文件 hadoop@ubuntu16:~$ vi .bashrc 6.2. 编辑文件，添加如下 #set hadoop env begin export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$JAVA_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin #set hadoop env end 6.3. 使文件修改生效 hadoop@ubuntu16:~$ source .bashrc 6.4. 查看hadoop版本信息（注意：与上一步骤中查看版本信息的区别） hadoop@ubuntu16:~$ hadoop version Hadoop 2.8.0 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009 Compiled by jdu on 2017-03-17T04:12Z Compiled with protoc 2.5.0 From source with checksum 60125541c2b3e266cbf3becc5bda666 This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.0.jar 配置hadoop伪分布式配置hadoop伪分布式需要修改相关配置文件：hadoop-env.xml/core-site.xml/hdfs-site.xml hadoop配置文件位置：hadoop主目录下的/etc/hadoop,如下 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ ls capacity-scheduler.xml httpfs-env.sh mapred-env.sh configuration.xsl httpfs-log4j.properties mapred-queues.xml.template container-executor.cfg httpfs-signature.secret mapred-site.xml core-site.xml httpfs-site.xml mapred-site.xml.template hadoop-env.cmd kms-acls.xml slaves hadoop-env.sh kms-env.sh ssl-client.xml.example hadoop-metrics.properties kms-log4j.properties ssl-server.xml.example hadoop-metrics2.properties kms-site.xml yarn-env.cmd hadoop-policy.xml log4j.properties yarn-env.sh hdfs-site.xml mapred-env.cmd yarn-site.xml 修改相应的配置文件 打开hadoop-env.sh hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi hadoop-env.sh 找到以下位置，把java目录修改为自己的主目录 # The java implementation to use. export JAVA_HOME=${JAVA_HOME} 打开core-site.xml,并添加configuration内容 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 打开hdfs-site.xml,并添加configuration内容hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 格式化namenode hadoop@ubuntu16:~$ hdfs namenode -format 启动NameNode和DataNode进程 hadoop@ubuntu16:~$ start-dfs.sh Starting namenodes on [localhost] localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-ubuntu16.out localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-ubuntu16.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-ubuntu16.out hadoop@ubuntu16:~$ jps 41749 NameNode 42203 Jps 42092 SecondaryNameNode 41903 DataNode 访问web界面输入主机ip:50070,访问主界面，如下 运行hadoop实例 创建目录hadoop@ubuntu16:~$ hdfs dfs -mkdir /user/hadoop hadoop@ubuntu16:~$ hdfs dfs -mkdir input 将本地文件拷贝到hdfs下input目录（发现有警告信息，目前还没有找到解决方法，不管不影响运行结果） hadoop@ubuntu16:~$ hdfs dfs -put /usr/local/hadoop/etc/hadoop/*.xml input 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) hadoop@ubuntu16:~$ hdfs dfs -ls Found 1 items drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:00 input hadoop@ubuntu16:~$ hdfs dfs -ls input Found 8 items -rw-r--r-- 1 hadoop supergroup 4942 2017-07-08 16:00 input/capacity-scheduler.xml -rw-r--r-- 1 hadoop supergroup 1032 2017-07-08 16:00 input/core-site.xml -rw-r--r-- 1 hadoop supergroup 9683 2017-07-08 16:00 input/hadoop-policy.xml -rw-r--r-- 1 hadoop supergroup 1079 2017-07-08 16:00 input/hdfs-site.xml -rw-r--r-- 1 hadoop supergroup 620 2017-07-08 16:00 input/httpfs-site.xml -rw-r--r-- 1 hadoop supergroup 3518 2017-07-08 16:00 input/kms-acls.xml -rw-r--r-- 1 hadoop supergroup 5546 2017-07-08 16:00 input/kms-site.xml -rw-r--r-- 1 hadoop supergroup 794 2017-07-08 16:00 input/yarn-site.xml 运行自带实例 hadoop@ubuntu16:~$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.0.jar grep input output ‘dfs[a-z.]+’ 查看运行结果 hadoop@ubuntu16:~$ hdfs dfs -ls Found 2 items drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:00 input drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:10 output hadoop@ubuntu16:~$ hdfs dfs -ls output Found 2 items -rw-r--r-- 1 hadoop supergroup 0 2017-07-08 16:10 output/_SUCCESS -rw-r--r-- 1 hadoop supergroup 77 2017-07-08 16:10 output/part-r-00000 hadoop@ubuntu16:~$ hdfs dfs -cat output/* 1 dfsadmin 1 dfs.replication 1 dfs.namenode.name.dir 1 dfs.datanode.data.dir NOTE: 再次运行前，需要删除掉output目录，否则会报错 配置yarn 打开yarn-site.xml,并添加如下configuration &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动ResourceManager和NodeManager进程,jps可以看出比单独启动start-dfs.sh 多出来两个进程 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-ubuntu16.out localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-ubuntu16.out hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ jps 46788 SecondaryNameNode 51397 NodeManager 46598 DataNode 46443 NameNode 51724 Jps 51276 ResourceManager 启动历史进程hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ mr-jobhistory-daemon.sh start historyserver starting historyserver, logging to /usr/local/hadoop/logs/mapred-hadoop-historyserver-ubuntu16.out 访问web界面输入主机ip:8088,访问主界面，如下 关闭所有进程,和开启顺序相反 hadoop@ubuntu16:~$ mr-jobhistory-daemon.sh stop historyserver stopping historyserver hadoop@ubuntu16:~$ stop-yarn.sh stopping yarn daemons stopping resourcemanager localhost: stopping nodemanager localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9 no proxyserver to stop hadoop@ubuntu16:~$ stop-dfs.sh Stopping namenodes on [localhost] localhost: stopping namenode localhost: stopping datanode Stopping secondary namenodes [0.0.0.0] 0.0.0.0: stopping secondarynamenode]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install and Configure Oracle JDK]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2Fubuntu-install-jdk-without-source-everytime-open-a-new-shell%2F</url>
    <content type="text"><![CDATA[查看系统位数，终端输入： getconf LONG_BIT 下载对应版本的jdk，这里下载的是jdk-8u131-linux-x64.tar.gz 创建目录作为jdk安装目录，这里选择安装位置为：~/Java（可自行选择安装路径） sudo mkdir Java 解压文件到上一步创建的目录~/Java目录下,JDK默认下载路径为Downloads目录 cd ~/Downloads sudo tar -zxvf jdk-8u131-linux-x64.tar.gz -C ~/Java 配置系统环境变量(全局：/etc/profile|当前用户：~/.bashrc) sudo vi /etc/profile 在最后加入: ##config java environment start export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib:$CLASSPATH export PATH=${JAVA_HOME}/bin:${JAVA_HOME}/jre/bin:$PATH ##config java environment end 修改完成后，保存并关闭，输入一下命令使环境变量生效 source /etc/profile 查看安装版本： java -version 本以为到此就结束了，结果发现每次重新打开一个terminal，就找不到java环境，搜索发现做以下配置: 配置/etc/bash.bashrc sudo vi /etc/bash.bashrc 在最后加入： ##config java environment start export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib:$CLASSPATH export PATH=${JAVA_HOME}/bin:${JAVA_HOME}/jre/bin:$PATH ##config java environment end 改完成后，保存并关闭，输入一下命令使环境变量生效 source /etc/bash.bashrc 到此完成安装配置jdk。]]></content>
      <categories>
        <category>配置相关</category>
      </categories>
      <tags>
        <tag>jdk</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacPro Connect Ubuntu16.04 In VWware Fusion]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2FMac-Connect-Ubuntu-In-VWware-Fusion%2F</url>
    <content type="text"><![CDATA[Mac 和 Linux虚拟机互通开启ssh服务 查看是否安装ssh服务：ps -e | grep ssh 安装ssh服务：sudo apt-get install ssh mac终端ssh连接Linux虚拟机:ssh user@remote[ip] 为了方便使用别名sshubt登陆 编辑mac下.bash_profile文件(需要root权限):sudo vi ~/.bash_profile 添加下面语句 alias sshubt=&apos;ssh myusername@192.168.0.0&apos; 使用别名sshubt，输入连接到的Linux主机密码，登陆即可jockie:~$ sshubt johnathon@192.168.140.128&apos;s password: Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.8.0-36-generic x86_64) mac上传文件址Linux虚拟机 scp ~/local/file user@remtoe:~/file ~/local/file: mac文件路径 user@remote:~/file: 服务器文件路径]]></content>
      <categories>
        <category>配置相关</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mac</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[long time no see]]></title>
    <url>%2Funcategorized%2Flong-time-no-see%2F</url>
    <content type="text"><![CDATA[It’s been almost 3 years since I met you ,hexo! NOW,It’s Time to Come Back!!!]]></content>
  </entry>
</search>