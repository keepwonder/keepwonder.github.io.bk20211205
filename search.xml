<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Probles in Using Hive]]></title>
    <url>%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2Fprobles-in-using-hive%2F</url>
    <content type="text"><![CDATA[在使用beeline方式连接hive时，遇到的一个坑，困扰多时，在网上也搜了好久，还好没放弃，今天终于找到了答案，在此非常感谢 [Hive]那些年我们踩过的Hive坑(如有侵犯，还望告知),我的问题就是其中的第10个问题。 Question01 问题: Error: Could not open client transport with JDBC Uri: jdbc:hive2://192.168.140.128:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: hadoop is not allowed to impersonate hive (state=08S01,code=0) 解决方法:修改hadoop 配置文件 etc/hadoop/core-site.xml,加入如下配置项 &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;The superuser can connect only from host1 and host2 to impersonate a user&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;Allow the superuser oozie to impersonate any members of the group group1 and group2&lt;/description&gt; &lt;/property&gt; 需要注意的是：hadoop.proxyuser.?.hosts 和 hadoop.proxyuser.?.groups中的 ? 和报错信息 User: ? is not allowed to impersonate hive (state=08S01,code=0)中的? 相对应，我这里是hadoop,所以我填的是hadoop.]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install and Configure Hive2.1.1]]></title>
    <url>%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FInstall-and-Configure-Hive2-1-1%2F</url>
    <content type="text"><![CDATA[准备工作 安装jdk 参考Ubuntu16.04 Install and Configure Oracle JDK 安装Hadoop 参考Ubuntu16.04 Install Hadoop 2.8.0 下载hive安装包前往hive官方下载地址 解压安装hive 解压hive安装文件到相应的目录hadoop@ubuntu16:~$ sudo tar -zxf apache-hive-2.1.1-bin.tar.gz -C /usr/local 给hive目录重命名hadoop@ubuntu16:~$ cd /usr/local hadoop@ubuntu16:/usr/local$ sudo mv apache-hive-2.1.1-bin/ hive 将hive目录用户改为hadoophadoop@ubuntu16:/usr/local$ sudo chown -R hadoop hive/ 查看hive目录hadoop@ubuntu16:/usr/local$ cd hive/ hadoop@ubuntu16:/usr/local/hadoop$ ls -l hadoop@ubuntu16:/usr/local/hive$ ll total 112 drwxr-xr-x 10 hadoop root 4096 Jul 8 22:11 ./ drwxr-xr-x 12 root root 4096 Jul 8 20:11 ../ -rw-r--r-- 1 hadoop staff 29003 Nov 29 2016 LICENSE -rw-r--r-- 1 hadoop staff 578 Nov 29 2016 NOTICE -rw-r--r-- 1 hadoop staff 4122 Nov 29 2016 README.txt -rw-r--r-- 1 hadoop staff 18501 Nov 30 2016 RELEASE_NOTES.txt drwxr-xr-x 3 hadoop root 4096 Jul 8 20:11 bin/ drwxr-xr-x 3 hadoop root 4096 Jul 8 22:15 conf/ drwxr-xr-x 4 hadoop root 4096 Jul 8 20:11 examples/ drwxr-xr-x 7 hadoop root 4096 Jul 8 20:11 hcatalog/ drwxr-xr-x 2 hadoop root 4096 Jul 8 20:11 jdbc/ drwxr-xr-x 4 hadoop root 12288 Jul 8 21:10 lib/ drwxr-xr-x 4 hadoop root 4096 Jul 8 20:11 scripts/ drwxrwxr-x 3 hadoop hadoop 4096 Jul 8 22:19 tmp/ 设置hive环境变量 打开~/.bashrc文件，并添加如下 # set hive env start export HIVE_HOME=/usr/local/hive export PATH=$PATH:$HIVE_HOME/bin export PATH=$PATH:$HIVE_HOME/hcatalog/bin:$HIVE_HOME/hcatalog/sbin export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib export HIVE_CONF_DIR=$HIVE_HOME/conf # set hive env end 使配置文件生效 hadoop@ubuntu16:~$ source ~/.bashrc 配置hive 修改默认配置文件名使配置文件生效 hadoop@ubuntu16:~$ cd /usr/local/hive/conf/ hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-env.sh.template hive-env.sh hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-default.xml.template hive-site.xml hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-log4j2.properties.template hive-log4j2.properties hadoop@ubuntu16:/usr/local/hive/conf$ cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties 修改hive-env.sh,添加如下 export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export HADOOP_HOME=/usr/local/hadoop export HIVE_HOME=/usr/local/hive export HIVE_CONF_DIR=/usr/local/hive/conf 创建hdfs目录 hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/warehouse hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/tmp hadoop@ubuntu16:~$ hdfs dfs -mkdir -p /user/hive/log hadoop@ubuntu16:~$ hdfs dfs -chmod -R 777 /user/hive/warehouse hadoop@ubuntu16:~$ hdfs DFS -chmod -R 777 /user/hive/tmp hadoop@ubuntu16:~$ hdfs dfs -chmod -R 777 /user/hive/log hadoop@ubuntu16:~$ hdfs dfs -ls /user/hive Found 3 items drwxrwxrwx - hadoop supergroup 0 2017-07-08 20:50 /user/hive/log drwxrwxrwx - hadoop supergroup 0 2017-07-08 21:54 /user/hive/tmp drwxrwxrwx - hadoop supergroup 0 2017-07-08 20:50 /user/hive/warehouse 本地建立tmp目录 hadoop@ubuntu16:/usr/local/hive$ mkdir tmp 安装mysql数据库，并作相关配置获取最近的软件包的列表 hadoop@ubuntu16:~$ sudo apt-get update 安装mysql服务和客户端，中间会要求输入root密码 hadoop@ubuntu16:~$ sudo apt-get install mysql-server mysql-client root登陆mysql hadoop@ubuntu16:~$ mysql -u root -p 创建hive用户 mysql&gt; create user &apos;hive&apos; identified by &apos;hive&apos;; 查看数据库 mysql&gt; show databases; 创建数据库命名为hive mysql&gt; create database hive; 为hive用户授权 mysql&gt; grant all privileges on *.* to &apos;hive&apos;@&apos;localhost&apos; identified by &apos;hive&apos;; mysql&gt; flush privileges 退出root登陆 mysql&gt; exit; hive用户登录 hadoop@ubuntu16:~$ mysql -u hive -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 67 Server version: 5.7.18-0ubuntu0.16.04.1 (Ubuntu) Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; 修改hive-site.xml文件6.1 相关目录信息 &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/user/hive/tmp&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/user/hive/log&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; 6.2 ${system:java.io.tmpdir} 和 ${system:user.name} 分别替换成 /user/local/hive/tmp 和 ${user.name}6.3 mysql数据库连接信息,需要将mysql的jar包放入hive/lib目录下 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; 启动hive hadoop@ubuntu16:~$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in file:/usr/local/hive/conf/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. hive&gt;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>mysql</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Command Line Series：A Command A Day 001]]></title>
    <url>%2FLinux%2Fa-command-a-day-001%2F</url>
    <content type="text"><![CDATA[Day001：help在我们遇到一个命令，不知道其具体用法的时候，help命令可能是我们使用的最多也最方便的命令了。 johnathon@ubuntu16:~$ help help help: help [-dms] [pattern ...] Display information about builtin commands. Displays brief summaries of builtin commands. If PATTERN is specified, gives detailed help on all commands matching PATTERN, otherwise the list of help topics is printed. Options: -d output short description for each topic -m display usage in pseudo-manpage format -s output only a short usage synopsis for each topic matching PATTERN Arguments: PATTERN Pattern specifiying a help topic Exit Status: Returns success unless PATTERN is not found or an invalid option is given. 简要的解释一下打印出来的信息 help: help [-dms] [pattern …] help命令后面可以跟[选项][参数]，可以看到选项和参数都是中括号扩起来的，说明都是可选的，也就意味着，可以直接输入help Display information about builtin commands. 直接翻译过来就是：显示内置命令信息 Displays brief summaries of builtin commands. If PATTERN is specified, gives detailed help on all commands matching PATTERN, otherwise the list of help topics is printed. 直接翻译过来就是：显示内置命令的简要概括信息。指定了PATTERN,就会给出所有符这个PATTERN命令的帮助信息，否则打印出help主题的列表信息 举例：pattern为p开头的内置命令,就显示出了popd/printf/pushd/pwd四个命令 johnathon@ubuntu16:~$ help p* Shell commands matching keyword `p*&apos; popd: popd [-n] [+N | -N] Remove directories from stack. Removes entries from the directory stack. With no arguments, removes the top directory from the stack, and changes to the new top directory. Options: -n Suppresses the normal change of directory when removing directories from the stack, so only the stack is manipulated. Arguments: +N Removes the Nth entry counting from the left of the list shown by `dirs&apos;, starting with zero. For example: `popd +0&apos; removes the first directory, `popd +1&apos; the second. -N Removes the Nth entry counting from the right of the list shown by `dirs&apos;, starting with zero. For example: `popd -0&apos; removes the last directory, `popd -1&apos; the next to last. The `dirs&apos; builtin displays the directory stack. Exit Status: Returns success unless an invalid argument is supplied or the directory change fails. printf: printf [-v var] format [arguments] Formats and prints ARGUMENTS under control of the FORMAT. Options: -v var assign the output to shell variable VAR rather than display it on the standard output FORMAT is a character string which contains three types of objects: plain characters, which are simply copied to standard output; character escape sequences, which are converted and copied to the standard output; and format specifications, each of which causes printing of the next successive argument. In addition to the standard format specifications described in printf(1), printf interprets: %b expand backslash escape sequences in the corresponding argument %q quote the argument in a way that can be reused as shell input %(fmt)T output the date-time string resulting from using FMT as a format string for strftime(3) The format is re-used as necessary to consume all of the arguments. If there are fewer arguments than the format requires, extra format specifications behave as if a zero value or null string, as appropriate, had been supplied. Exit Status: Returns success unless an invalid option is given or a write or assignment error occurs. pushd: pushd [-n] [+N | -N | dir] Add directories to stack. Adds a directory to the top of the directory stack, or rotates the stack, making the new top of the stack the current working directory. With no arguments, exchanges the top two directories. Options: -n Suppresses the normal change of directory when adding directories to the stack, so only the stack is manipulated. Arguments: +N Rotates the stack so that the Nth directory (counting from the left of the list shown by `dirs&apos;, starting with zero) is at the top. -N Rotates the stack so that the Nth directory (counting from the right of the list shown by `dirs&apos;, starting with zero) is at the top. dir Adds DIR to the directory stack at the top, making it the new current working directory. The `dirs&apos; builtin displays the directory stack. Exit Status: Returns success unless an invalid argument is supplied or the directory change fails. pwd: pwd [-LP] Print the name of the current working directory. Options: -L print the value of $PWD if it names the current working directory -P print the physical directory, without any symbolic links By default, `pwd&apos; behaves as if `-L&apos; were specified. Exit Status: Returns 0 unless an invalid option is given or the current directory cannot be read. 没有符合的pattern： johnathon@ubuntu16:~$ help haha* Shell commands matching keyword `haha*&apos; -bash: help: no help topics match `haha*&apos;. Try `help help&apos; or `man -k haha*&apos; or `info haha*&apos;. Options[选项]:-d output short description for each topic-m display usage in pseudo-manpage format-s output only a short usage synopsis for each topic matching PATTERN -d: 输出简短介绍信息 johnathon@ubuntu16:~$ help -d help help - Display information about builtin commands. -m: 以伪man页面格式显示使用信息 johnathon@ubuntu16:~$ help -m help NAME help - Display information about builtin commands. SYNOPSIS help [-dms] [pattern ...] DESCRIPTION Display information about builtin commands. Displays brief summaries of builtin commands. If PATTERN is specified, gives detailed help on all commands matching PATTERN, otherwise the list of help topics is printed. Options: -d output short description for each topic -m display usage in pseudo-manpage format -s output only a short usage synopsis for each topic matching PATTERN Arguments: PATTERN Pattern specifiying a help topic Exit Status: Returns success unless PATTERN is not found or an invalid option is given. SEE ALSO bash(1) IMPLEMENTATION GNU bash, version 4.3.48(1)-release (x86_64-pc-linux-gnu) Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; -s: 打印出help命令的梗概信息 johnathon@ubuntu16:~$ help -s help help: help [-dms] [pattern ...] Arguments[参数]: 指定需要帮助的主题的pattern Exit Status:Returns success unless PATTERN is not found or an invalid option is given.返回成功状态，除非pattern没有找到，或者输入了无效的选项]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install Hadoop 2.8.0]]></title>
    <url>%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FUbuntu16-04-Install-Hadoop-2-8-0%2F</url>
    <content type="text"><![CDATA[创建hadoop用户 创建用户hadoop,并且用/bin/bash作为默认shell：johnathon@ubuntu16:~$ sudo useradd -m hadoop -s /bin/bash 为用户hadoop设置密码：johnathon@ubuntu16:~$ sudo passwd hadoop 将用户hadoop加入sudo组：johnathon@ubuntu16:~$ sudo adduser hadoop sudo 更新apt hadoop@ubuntu16:~$ sudo apt-get update 配置ssh免密登陆 安装ssh服务hadoop@ubuntu16:~$ sudo apt-get install ssh 创建.ssh目录hadoop@ubuntu16:~$ cd ~ hadoop@ubuntu16:~$ mkdir .ssh 生成ssh密钥hadoop@ubuntu16:~$ cd .ssh/ hadoop@ubuntu16:~$ ssh-keygen -t rsa hadoop@ubuntu16:~/.ssh$ cat id_rsa.pub &gt;&gt; authorized_keys ssh登陆localhosthadoop@ubuntu16:~/.ssh$ cd hadoop@ubuntu16:~$ ssh localhost 退出ssh登陆hadoop@ubuntu16:~$ exit logout Connection to localhost closed. 安装配置jdk 参考Ubuntu16.04 Install and Configure Oracle JDK 安装hadoop 解压hadoop安装文件到相应的目录hadoop@ubuntu16:~$ sudo tar -zxf hadoop-2.8.0.tar.gz -C /usr/local 给hadoop目录重命名hadoop@ubuntu16:~$ cd /usr/local hadoop@ubuntu16:/usr/local$ sudo mv hadoop-2.8.0/ hadoop 将hadoop目录用户改为hadoophadoop@ubuntu16:/usr/local$ sudo chown -R hadoop hadoop/ 查看hadoop目录hadoop@ubuntu16:/usr/local$ cd hadoop/ hadoop@ubuntu16:/usr/local/hadoop$ ls -l total 148 -rw-r--r-- 1 hadoop dialout 99253 Mar 17 13:31 LICENSE.txt -rw-r--r-- 1 hadoop dialout 15915 Mar 17 13:31 NOTICE.txt -rw-r--r-- 1 hadoop dialout 1366 Mar 17 13:31 README.txt drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 bin drwxr-xr-x 3 hadoop dialout 4096 Mar 17 13:31 etc drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 include drwxr-xr-x 3 hadoop dialout 4096 Mar 17 13:31 lib drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 libexec drwxr-xr-x 2 hadoop dialout 4096 Mar 17 13:31 sbin drwxr-xr-x 4 hadoop dialout 4096 Mar 17 13:31 share 查看hadoop版本信息 hadoop@ubuntu16:/usr/local/hadoop$ ./bin/hadoop version Hadoop 2.8.0 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009 Compiled by jdu on 2017-03-17T04:12Z Compiled with protoc 2.5.0 From source with checksum 60125541c2b3e266cbf3becc5bda666 This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.0.jar 设置hadoop环境变量 6.1. 打开hadoop用户下配置文件 hadoop@ubuntu16:~$ vi .bashrc 6.2. 编辑文件，添加如下 #set hadoop env begin export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$JAVA_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin #set hadoop env end 6.3. 使文件修改生效 hadoop@ubuntu16:~$ source .bashrc 6.4. 查看hadoop版本信息（注意：与上一步骤中查看版本信息的区别） hadoop@ubuntu16:~$ hadoop version Hadoop 2.8.0 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009 Compiled by jdu on 2017-03-17T04:12Z Compiled with protoc 2.5.0 From source with checksum 60125541c2b3e266cbf3becc5bda666 This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.8.0.jar 配置hadoop伪分布式配置hadoop伪分布式需要修改相关配置文件：hadoop-env.xml/core-site.xml/hdfs-site.xml hadoop配置文件位置：hadoop主目录下的/etc/hadoop,如下 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ ls capacity-scheduler.xml httpfs-env.sh mapred-env.sh configuration.xsl httpfs-log4j.properties mapred-queues.xml.template container-executor.cfg httpfs-signature.secret mapred-site.xml core-site.xml httpfs-site.xml mapred-site.xml.template hadoop-env.cmd kms-acls.xml slaves hadoop-env.sh kms-env.sh ssl-client.xml.example hadoop-metrics.properties kms-log4j.properties ssl-server.xml.example hadoop-metrics2.properties kms-site.xml yarn-env.cmd hadoop-policy.xml log4j.properties yarn-env.sh hdfs-site.xml mapred-env.cmd yarn-site.xml 修改相应的配置文件 打开hadoop-env.sh hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi hadoop-env.sh 找到以下位置，把java目录修改为自己的主目录 # The java implementation to use. export JAVA_HOME=${JAVA_HOME} 打开core-site.xml,并添加configuration内容 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 打开hdfs-site.xml,并添加configuration内容hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ vi hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 格式化namenode hadoop@ubuntu16:~$ hdfs namenode -format 启动NameNode和DataNode进程 hadoop@ubuntu16:~$ start-dfs.sh Starting namenodes on [localhost] localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-ubuntu16.out localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-ubuntu16.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-ubuntu16.out hadoop@ubuntu16:~$ jps 41749 NameNode 42203 Jps 42092 SecondaryNameNode 41903 DataNode 访问web界面输入主机ip:50070,访问主界面，如下 运行hadoop实例 创建目录hadoop@ubuntu16:~$ hdfs dfs -mkdir /user/hadoop hadoop@ubuntu16:~$ hdfs dfs -mkdir input 将本地文件拷贝到hdfs下input目录（发现有警告信息，目前还没有找到解决方法，不管不影响运行结果） hadoop@ubuntu16:~$ hdfs dfs -put /usr/local/hadoop/etc/hadoop/*.xml input 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) 17/07/08 16:00:09 WARN hdfs.DataStreamer: Caught exception java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:1252) at java.lang.Thread.join(Thread.java:1326) at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:927) at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:578) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:755) hadoop@ubuntu16:~$ hdfs dfs -ls Found 1 items drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:00 input hadoop@ubuntu16:~$ hdfs dfs -ls input Found 8 items -rw-r--r-- 1 hadoop supergroup 4942 2017-07-08 16:00 input/capacity-scheduler.xml -rw-r--r-- 1 hadoop supergroup 1032 2017-07-08 16:00 input/core-site.xml -rw-r--r-- 1 hadoop supergroup 9683 2017-07-08 16:00 input/hadoop-policy.xml -rw-r--r-- 1 hadoop supergroup 1079 2017-07-08 16:00 input/hdfs-site.xml -rw-r--r-- 1 hadoop supergroup 620 2017-07-08 16:00 input/httpfs-site.xml -rw-r--r-- 1 hadoop supergroup 3518 2017-07-08 16:00 input/kms-acls.xml -rw-r--r-- 1 hadoop supergroup 5546 2017-07-08 16:00 input/kms-site.xml -rw-r--r-- 1 hadoop supergroup 794 2017-07-08 16:00 input/yarn-site.xml 运行自带实例 hadoop@ubuntu16:~$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.0.jar grep input output ‘dfs[a-z.]+’ 查看运行结果 hadoop@ubuntu16:~$ hdfs dfs -ls Found 2 items drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:00 input drwxr-xr-x - hadoop supergroup 0 2017-07-08 16:10 output hadoop@ubuntu16:~$ hdfs dfs -ls output Found 2 items -rw-r--r-- 1 hadoop supergroup 0 2017-07-08 16:10 output/_SUCCESS -rw-r--r-- 1 hadoop supergroup 77 2017-07-08 16:10 output/part-r-00000 hadoop@ubuntu16:~$ hdfs dfs -cat output/* 1 dfsadmin 1 dfs.replication 1 dfs.namenode.name.dir 1 dfs.datanode.data.dir NOTE: 再次运行前，需要删除掉output目录，否则会报错 配置yarn 打开yarn-site.xml,并添加如下configuration &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动ResourceManager和NodeManager进程,jps可以看出比单独启动start-dfs.sh 多出来两个进程 hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-ubuntu16.out localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-ubuntu16.out hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ jps 46788 SecondaryNameNode 51397 NodeManager 46598 DataNode 46443 NameNode 51724 Jps 51276 ResourceManager 启动历史进程hadoop@ubuntu16:/usr/local/hadoop/etc/hadoop$ mr-jobhistory-daemon.sh start historyserver starting historyserver, logging to /usr/local/hadoop/logs/mapred-hadoop-historyserver-ubuntu16.out 访问web界面输入主机ip:8088,访问主界面，如下 关闭所有进程,和开启顺序相反 hadoop@ubuntu16:~$ mr-jobhistory-daemon.sh stop historyserver stopping historyserver hadoop@ubuntu16:~$ stop-yarn.sh stopping yarn daemons stopping resourcemanager localhost: stopping nodemanager localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9 no proxyserver to stop hadoop@ubuntu16:~$ stop-dfs.sh Stopping namenodes on [localhost] localhost: stopping namenode localhost: stopping datanode Stopping secondary namenodes [0.0.0.0] 0.0.0.0: stopping secondarynamenode]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 Install and Configure Oracle JDK]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2Fubuntu-install-jdk-without-source-everytime-open-a-new-shell%2F</url>
    <content type="text"><![CDATA[查看系统位数，终端输入： getconf LONG_BIT 下载对应版本的jdk，这里下载的是jdk-8u131-linux-x64.tar.gz 创建目录作为jdk安装目录，这里选择安装位置为：~/Java（可自行选择安装路径） sudo mkdir Java 解压文件到上一步创建的目录~/Java目录下,JDK默认下载路径为Downloads目录 cd ~/Downloads sudo tar -zxvf jdk-8u131-linux-x64.tar.gz -C ~/Java 配置系统环境变量(全局：/etc/profile|当前用户：~/.bashrc) sudo vi /etc/profile 在最后加入: ##config java environment start export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib:$CLASSPATH export PATH=${JAVA_HOME}/bin:${JAVA_HOME}/jre/bin:$PATH ##config java environment end 修改完成后，保存并关闭，输入一下命令使环境变量生效 source /etc/profile 查看安装版本： java -version 本以为到此就结束了，结果发现每次重新打开一个terminal，就找不到java环境，搜索发现做以下配置: 配置/etc/bash.bashrc sudo vi /etc/bash.bashrc 在最后加入： ##config java environment start export JAVA_HOME=/home/johnathon/Java/jdk1.8.0_131 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib:$CLASSPATH export PATH=${JAVA_HOME}/bin:${JAVA_HOME}/jre/bin:$PATH ##config java environment end 改完成后，保存并关闭，输入一下命令使环境变量生效 source /etc/bash.bashrc 到此完成安装配置jdk。]]></content>
      <categories>
        <category>配置相关</category>
      </categories>
      <tags>
        <tag>jdk</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacPro Connect Ubuntu16.04 In VWware Fusion]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2FMac-Connect-Ubuntu-In-VWware-Fusion%2F</url>
    <content type="text"><![CDATA[Mac 和 Linux虚拟机互通开启ssh服务 查看是否安装ssh服务：ps -e | grep ssh 安装ssh服务：sudo apt-get install ssh mac终端ssh连接Linux虚拟机:ssh user@remote[ip] 为了方便使用别名sshubt登陆 编辑mac下.bash_profile文件(需要root权限):sudo vi ~/.bash_profile 添加下面语句 alias sshubt=&apos;ssh myusername@192.168.0.0&apos; 使用别名sshubt，输入连接到的Linux主机密码，登陆即可jockie:~$ sshubt johnathon@192.168.140.128&apos;s password: Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.8.0-36-generic x86_64) mac上传文件址Linux虚拟机 scp ~/local/file user@remtoe:~/file ~/local/file: mac文件路径 user@remote:~/file: 服务器文件路径]]></content>
      <categories>
        <category>配置相关</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>Ubuntu</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[long time no see]]></title>
    <url>%2Funcategorized%2Flong-time-no-see%2F</url>
    <content type="text"><![CDATA[It’s been almost 3 years since I met you ,hexo! NOW,It’s Time to Come Back!!!]]></content>
  </entry>
</search>